---
{"dg-publish":true,"permalink":"/日常学习/技术学习/大模型从零开始/","title":"大模型从零开始","tags":["LLM","八股"],"noteIcon":"1","created":"2024-03-09T19:14:40.497+08:00","updated":"2024-11-06T20:42:46.687+08:00"}
---

<div id="summary">
文章详细介绍了大模型的结构与优化方法，涵盖注意力机制、Encoder-Decoder 结构、BatchNorm 和 LayerNorm 的区别，以及常用的优化方法。文章还深入探讨了Bert模型及其变体，预训练与微调阶段的知识注入方法。除此之外，还涉及RAG检索增强生成、LangChain与向量数据库、思维链、强化学习RLHF、分布式训练和Agent等相关内容，提供了丰富的技术细节与实践经验。
</div>

{% quot 本文大量内容整理自网络，侵删。 %}

## 模型结构

> [!summary] 本质上这类问题是考基础，现有模型都是在标准的 Transformer 结构上修补。

### 基础概念

- 梯度消失：反向传播中，若某些层的梯度小于 1，随着层数的增加，梯度会指数级降低，最终靠近输入层时变得极小，导致权重更新缓慢，难以学习和优化。（e.g. 选用 Sigmoid 函数作为损失函数）
- 梯度爆炸：反向传播中，网络梯度值逐层放大，最终靠近输入层时变得极大，导致网络前层权重更新幅度大，难以稳定训练，甚至无法收敛。（e.g. ReLU 函数且权重较大时）
- FLOPS：等同于 FLOP/s，表示 Floating Point Operations Per Second，即每秒执行的浮点数操作次数，用于衡量硬件计算性能。
- FLOPs：表示 Floating Point Operations，表示某个算法的总计算量（即总浮点运算次数），用于衡量一个算法的复杂度。

### [注意力机制](https://zhuanlan.zhihu.com/p/455399791)

- 为什么要有注意力机制
    - RNN 中 token 是一个个喂给模型的，随着序列长度的增加，模型下一步计算的等待时间越长，无法实现并行计算，而远距离间信息缺失情况也越明显。
    - 编码器中的自注意力机制：用于输入序列的全局信息交互和依赖关系建模，每个位置都可以与其它位置进行信息交换。
    - 解码器中的掩码自注意力机制：用于生成序列中的当前和之前位置的信息交互，防止生成时泄露未来的信息。
    - 解码器中的交叉注意力机制：结合编码器输出和解码器输入（Q 来自解码器当前层的输入，K 和 V 来自编码器输出），生成新的序列，使解码器能够利用编码器的全局上下文信息。
- 注意力机制的计算公式，一定要这样计算吗
- Transformer 为什么使用多头注意力机制，分析 [时间复杂度](https://zhuanlan.zhihu.com/p/514109726)
    - 捕捉多样化的特征，在不同子空间中学习不同的特征，多角度建模增强表达能力。
    - 增强模型的稳定性和泛化能力，避免单一的注意力头过拟合某些特定特征。
    - 并行计算，提高计算效率，尤其是在 GPU 环境下。
    - 提高上下文信息的捕捉能力，关注输入序列的不同部分。
    - 时间复杂度大致和单头的相似，均为 $O(n^2d)$，在不同子空间中并行地处理输入序列，从而捕捉更丰富的特征，提高模型的表现力和稳定性。
- 为什么 Q 和 K 使用不同权重矩阵生成，为什么不能使用同一个值进行自身点乘
    - 每个词的 query 和 key 具有不同的含义，它们需要不同的权重矩阵来捕捉不同的信息，也就是在不同的空间投影，可以让模型更加灵活地调整每个位置对其他位置的关注度，从而提高模型的表达能力。如果 Q 和 K 一样，乘积结果矩阵中，对角线的值会比较大，导致每个词会过分关注自身，从而降低模型的表达泛化能力。
- 计算注意力为什么选择点乘而不是加法，从计算复杂度和效果上面讲区别
    - 二者复杂度都是 $O(d)$，前者可以利用矩阵乘法的硬件优化，后者需要额外经过 softmax 函数处理，增加了额外的复杂度。并且点乘可以自然度量相似性，加法捕捉向量间关系的能力较差。
- 为什么要对注意力进行 scaled（$\sqrt{d_k}$），公式推导
    - 之所以进行 scaling，是为了使得方差稳定（具体来说就是控制为 1），数据分布相对均匀，进而在 softmax 的过程中，梯度下降得更加稳定，避免因为梯度过小而造成模型参数更新的停滞。
- 在计算注意力时如何对 padding 做 mask 操作
    - pad 位置的注意力分数一般使用极大负数填充，后续 softmax 应用后这些位置的权重趋于零。
- 为什么在进行多头注意力时需要对每个 head 降维
    - 提高处理效率，避免过拟合（每个训练头都只表示一部分特征信息），最终输出时多头结果会被合并，仍旧保有原来的表达能力。
- transformer 是如何处理可变长数据的
    - RNN 是通过 timestep 的方式处理可变长数据，Transformer 是通过计算长度相关的 self-attention 得分矩阵来处理可变长数据。
- Flash Attention(Fast and Memory Efficient Exact Attention with IO-Awareness)
	- [v1](https://zhuanlan.zhihu.com/p/669926191)
		- 时间上计算瓶颈不仅仅在计算能力，而**主要**在**读写速度**，利用分块计算和核函数融合来降低对显存的访问，尽可能打满 SRAM，减少从显存搬运到 SRAM 计算的总时间。
		- 空间上节省显存，标准 attention 场景需要保存和读取 $N*N$ 的注意力矩阵，Flash Attention 将存储压力降至 $O(N)$。
		- 精准注意力计算，Flash Attention 确保了完全等同于标准 attention 的实现方式，比之前的近似方法更优。
		- 计算限制与内存限制
			- 判断方式：算法所需总运算量和总数据读取存储量的比值，与硬件算力上限和带宽的比值作比较，前者更大是计算限制主导，反之是内存限制。
			- 计算限制：大矩阵乘法、通道数很大的卷积运算。读更快，算更慢。
			- 内存限制：逐点运算操作。激活函数、dropout、mask、softmax、BN 和 LN 等。算更快，读更慢。
		- 分块计算：将 $Q$ 切分，然后 $K^T$ 和 $V$ 也切分（二者需要相同块数），本质是分块是为了减少 attention 计算期间的读写量，主要是 $S=QK^T$ 和 $P=softmax(S)$ 两个部分，也因此可以将显存占用从 $O(N^2)$ 降到 $O(N)$，计算量则为 $O(N^2d)$。其中需要注意分块导致的 $safe$ $softmax$ 局部最大、全局最大以及局部和、全局和的细节处理，最终输出结果也需要每遍历一块就不断用当前最新的 rowmax 和 rowsum 去更新，直到遍历完最后一块，确保和标准场景下结果完全一致。
		- 标准 attention 的 IO 复杂度是 $O(Nd+N^2)$，分块后的 IO 复杂度是 $O(\cfrac{N}{B_c}Nd)=O(\cfrac{4N^2d^2}{M})$，显著降低。
		- 数据块越大，读写次数越小，runtime 整体下降，但当数据库大小超过 256 后，runtime 下降不明显，因为矩阵变大导致计算耗时更大，平衡了读写节省的时间。
	- [v2](https://zhuanlan.zhihu.com/p/691067658)
		- 交换内外循环，v1 使用 $KV$ 为外循环，$Q$ 为内循环，但其实 $softmax$ 也是在行维度的，所以固定 $Q$ 循环 $KV$ 更自然，计算输出合并时也更直接，节省了中间结果读取。
		- 新增 `seq_len` 维度的并行，尽可能打满 SM（流式多处理器）利用率。主要在 $Q$ 上切分，除非 SM 实在打不满否则不在 $KV$ 上切分，以确保不同的 block 之间可以独立计算（切分后每一行来自不同的 block，为了得到全局 $softmax$ 结果还需要汇总计算一次）。
		- 优化 block 内部 wrap 级别的工作模式，尽量减少 warp 间的通讯和读取 shared memory 的次数，例如之前需要反复读取 $KV$ 的数据，现在只用反复读取 $Q$ 的数据。
- Multi Query Attention 和 Group Query Attention
    - MQA 和 GQA 都是节省 KV Cache 显存，前者是所有注意力头都共享同样的 K、V，后者是在原始的 MHA 和共享的 MQA 之间取权衡。
    - KV Cache：Attention 每层的计算仅需要当前 $Q_k$ 参与，而 K 和 V 全程参与计算，因此需要将每一步的 K 和 V 缓存起来。占用大小计算：$b(s+n)h*l*2*2=4blh(s+n)$，其中 $b$ 是批大小，$s+n$ 是输入输出长度，$h$ 是注意力头数，$l$ 是层数，两个 $2$ 分别是 KV 以及 Float16 占 2 个 byte。

### Transformer

- Transformer 的两个缺点
    - 内存占用大：Transformer 的内存占用量随上下文长度而变化。这使得在没有大量硬件资源的情况下运行长上下文窗口或大量并行批处理变得具有挑战性，从而限制了广泛的实验和部署。
    - 随着上下文长度的增加，推理速度会变慢：Transformer 的注意力机制随序列长度呈二次方扩展，并且会降低吞吐量，因为每个 token 都依赖于它之前的整个序列，从而将长上下文用例置于高效生产的范围之外。
- 讲解一下 Encoder 的构造
	- 输入嵌入、多头自注意力机制计算 (MHA) 和前馈神经网络（FFN）。
	- 此外，每一层的自注意力机制和前馈网络后都跟有 残差连接（Residual Connection） 和 层归一化（Layer Normalization）。
- 为什么在获取词向量后需要对矩阵乘以 embedding size 的开方
	- Embedding matrix 的初始化方式是 xavier init，这种方式的方差是 1/embedding size，因此乘以 embedding size 的开方使得 embedding matrix 的方差是 1，在这个 scale 下可能更有利于 embedding matrix 的收敛，也有利于匹配位置编码的数据范围。
- [位置编码](https://zhuanlan.zhihu.com/p/454482273)
    - Transformer 并行计算，每个 token 彼此独立，也就是输入是无序的，所以需要位置编码提供位置信息。
    - 位置编码的实现
        - 用整型标记：模型可能会遇到比训练序列更长的输入，不利于泛化，且长度越长编码值会很大。
        - 用 $[0, 1]$ 标记：当序列长度不同时，token 间的相对距离不一样。
        - 用二进制编码：编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的，无法使用位置向量来表示浮点型。
        - 需要有界又连续的函数：三角函数（sin）满足条件，通过频率控制 sin 函数的波长，越往右走（也就是越低位）波长越大，对变化越不敏感，也就是需要更高精度控制。但是仍然有一个问题，sin 是周期函数，因此从纵向（token 序列）来看，如果函数的频率偏大，引起波长偏短，则不同 t 下的位置向量可能出现重合的情况，所以在原论文中选用了一个非常小的值作为频率，尽可能拉长波长。
        - 使用 sin 已经实现了位置向量唯一、有界且可泛化（周期），现在还需要不同位置的位置向量可以通过线性变换得到。因此改为每个 token 位置向量中两两一组，sin 和 cos 交替出现，线性变换就由旋转实现， $\Delta t$ 时刻相当于旋转对应的角度。
    - 位置编码的性质
        - 能够表示相对位置，两个位置编码的点积仅取决于偏移量 $\Delta t$ 。
        - 位置编码的点积具备无向性（对称性），即 $PE_t^T * PE_{t+\Delta t} = PE_t^T * PE_{t-\Delta t}$ 。
        - 综上两条，也就是位置编码的点积只能表示距离，无法表示方向。
        - 同时位置编码的内积存在远程衰减，也就是相近的位置对当前位置影响更大，较远的影响更弱，反映局部关系。
    - 长度外推问题
	    - 大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降。例如，如果一个模型在训练时只使用了 512 个 token 的文本，那么在预测时如果输入超过 512 个 token，模型可能无法正确处理。这就限制了大模型在处理**长文本**或**多轮对话**等任务时的效果，而这正是热点问题之一。
    - [旋转位置编码 RoPE](https://mp.weixin.qq.com/s/dn8Pb80iRF9UkRn4vPOhHA)
	    - 原始位置编码的问题：经过 attention 层后，位置变量真正起作用的变成了 $PE^T_t * W_Q^T * W_K * PE_{t + \Delta t}$，相当于引入了线性变化，无法保持对称性、远距离衰减等良好性质。
	    - 思路：attention 层计算会破坏输入层位置编码的性质，那么如果在 attention 层中融入位置信息，就能保持性质不变。当 $\Delta t$ 较小时，拉近 $q_t$ 和 $k_{t+\Delta t}$ 的距离；较大时则拉远距离。
	    - 实现：直接定义一个逆时针正交旋转矩阵，相当于只旋转 $q$、$k$ 来保证绝对位置信息，同时维护其原始特征。对于多维，两两一组应用旋转变换即可，可以理解为不同运转速度的时钟。
	    - RoPE 的完整操作流程是：对于 token 序列中的每个词嵌入向量，首先计算其对应的 query 和 key 向量，然后对每个 token 位置都计算对应的旋转位置编码，接着对每个 token 位置的 query 和 key 向量的元素按照两两一组应用旋转变换，最后再计算 query 和 key 之间的内积得到 self-attention 的计算结果。
	    - 需要注意，在 $\theta$ 的选择上，若引入过大的基数 $\theta$，则可能导致旋转角度十分微弱，相近位置的向量几乎重合，位置信息带来的帮助被大幅稀释。
	    - 使用【小基数 + 短数据】的组合训练，那么旋转角度较大，并不是每一个点都能训练到，但是通常可以收敛（大多数两两组合都可以至少旋转一圈）；使用【大基数 + 长文本】的组合做训练，旋转角度较小，训练覆盖粒度较好，但是大部分圆盘都很难旋转满一圈（收敛）。因此一般选择先用【小基数 + 短数据】训练，然后【大基数 + 长文本】微调。
	    - NTK-RoPE（NTK：神经正切核）：对于靠前的组合（圆盘），更容易反映绝对位置的变化，因此尽可能让其学习到**绝对位置**信息，也就是突破 pretrain 看过的圆周部分（外推）。而对于靠后的组合（圆盘），对绝对位置变化不敏感，我们尽可能让其学习到**相对位置**信息，保持在 pretrain 看过的圆周部分，更加角度精细化训练（内插）。对于最后一个组合（圆盘），缩放因子 $\lambda = (\cfrac{S_{post}}{S_{pre}})^{\frac{d}{d-2}}$，$S$ 表示训练文本长度。
	    - RoPE 仍然存在外推问题，表示为推理超过训练长度的内容时模型崩坏，PPL 等指标显著上升，可以从 [进制转换的角度](https://zhuanlan.zhihu.com/p/675243992) 理解外推、内插与 NTK-aware Scaled RoPE。
		    - 位置线性内插（Position Interpolation，PI），将超出长度的样本线性缩放映射回原区间，每个位置都不再是整数，也即位置 $m'=\cfrac{mL}{L'}$。内插和外推一样也还是需要微调，因为模型没有在训练时处理过拥挤的位置关系，但是所需要的步数明显更少，因为很多场景下相对大小（序信息）更加重要，而且可以利用模型具备的泛化能力结合相对大小理解绝对大小信息，缺点是在处理相近的 token 时可能无法准确区分它们的顺序和位置，分布情况可能变得复杂不均，加大了模型的学习难度。
		    - 低频部分：指位置编码中使用的较低频率的成分。这些成分变化缓慢，对应着较大跨度的位置变化。低频部分主要用于捕捉**远距离的位置关系**。在序列中，低频成分可以帮助模型更好地理解长距离之间的依赖性。在外推的情况下无法处理这部分信息，影响模型的理解。
		    - 高频部分：指位置编码中使用的较高频率的成分。这些成分变化快，对应着序列中较短的距离。高频部分主要用于编码**邻近位置之间的关系**，即局部信息。这在短距离的关系中尤其有用，比如相邻的单词或符号之间的关联性。内插在此时由于拥挤，处理会随长度增长越来越困难。
		    - 直接外推会将外推压力集中在高位（m 较大）上，而位置内插则会将低位（m 较小）的表示变得更加稠密，所以 NTK-Aware Scaled RoPE 的思想就是**高频外推，低频内插**，这里选择的方案是在 $\theta_{base}$ 引入 $\lambda=\sqrt[(d-2)/2]{k}$，保持和内插一致，而 $d$ 很大时有 $\lambda \rightarrow 1$，相当于外推。这种方式下不微调就取得了不错的外推结果。
		    - NTK-by-Parts：定义波长代表 $d$ 维 RoPE 嵌入执行完整旋转（2$\pi$）所需的 token 长度。如果波长远小于上下文长度，不进行内插。如果波长等于或大于上下文长度（低频部分），只进行内插并避免外推（与 NTK-aware 不同，避免外推到越界值影响内插和比例因子）。如果是介于中间的长度，采用 NTK-aware 的方案。
		    - Dynamic NTK：每次前向传递中，位置嵌入都会更新比例因子 $s=max(1,\frac{l'}{L})$，其中 $l'$ 表示当前序列长度。
	- FIRE（Functional Interpolation for Relative Position Embeddings）
		- 渐进式插值（查询和键的相对距离除以查询位置，实现归一化）与可学习的映射函数（也就是非线性内插）。
	- [YaRN(Yet another RoPE extensioN)](https://zhuanlan.zhihu.com/p/683863159)
		- 使用了 NTK-by-Parts。
		- 在计算注意力时引入温度缩放能够带来改进，因此把两个旋转位置嵌入缩放 $\sqrt{\cfrac{1}{t}}$ 即可。另外，原论文通过在 LLama 拟合最低困惑度实验，得到了相应的表达式来计算温度的缩放系数，即 $\sqrt{\cfrac{1}{t}}=0.1ln(s)+1$，其中 $s$ 表示缩放因子，该式也表明温度常数和熵可能存在一定的通用关系。
	- [CoPE(Contextual Positional Encoding)](https://www.zhihu.com/question/657761483)
		- 本质上和其它的位置编码方式不冲突，相当于一种新的位置信息思路。这种动态位置编码的方式更贴合人实际的处理逻辑，往往并不会单独一个个处理 token，而是相对模糊关注某一个词或者词组维度，且切割方式是不固定的，所以在某些场景下能够获得更优性能。
		- 位置矩阵计算量过大，相当于 Attention 矩阵计算量，且由于是 token 定制，希望做到 Contextual Positon，所以很难借助预处理省略。想要避免两次 Attention 矩阵计算量，那么只能应用于位置和语义解耦的位置编码（ALiBi 等），而也需要一定的适配复杂度。
		- 难以兼容 RoPE，因为 RoPE 会改变 $q$ 和 $k$，而在位置矩阵确定前无法进行 RoPE，陷入死锁。另外实践上 RoPE 需要通过绝对位置来实现相对位置保证计算效率。
    - [Attention with Linear Biases(ALiBi)](https://zhuanlan.zhihu.com/p/642846676)
	    - 直接不添加位置编码，改为加一个静态的不学习的常数负值（实际就是相对位置距离乘上每个头的对应权重），能够加快训练速度，减小参数量，外推情况下更加稳定，但在实践中使用不够多，可能有训练稳定性问题（例如 Loss Spike），也有讨论说感受野太有限，远程衰减过于显著。
- [残差结构及其意义](https://zhuanlan.zhihu.com/p/459065530)
    - $H(x)=F(x)+x$ ， $H(x)$ 表示恒等映射，残差主要用于解决网络加深后出现的退化问题。通过加深网络可以尝试让不同层学习不同特征，进而增强模型表达能力，但是由于退化和梯度消失，需要输出逼近输入，这样就能**保证深网络的效果不会比浅网络更差**，也就是 $F(x)$ 残差逼近 0。
    - transformer 中，对每一层的 attention 和 FFN，都采用了一次残差连接，也即每个位置当前层的输入和输出相加。
- [BatchNorm 和 LayerNorm 的区别，为什么选择用 LayerNorm](https://zhuanlan.zhihu.com/p/456863215)
    - 层间输入分布偏移问题（ICS）
        - 由于数据分布的变化，导致每层的参数依次受到前面的变动影响，在适应过程中训练的难度增大。
        - 在过激活层的时候，容易陷入激活层的梯度饱和区，降低模型收敛速度。比如 sigmoid 函数在绝对值很大时梯度就几乎消失。
        - 输入变动大，上层网络需要不断去适应下层变动，因此学习率不能设置过大，因为每一步确定性很低，但是这进一步降低了模型收敛速度。
        - 而无论是采用非饱和激活函数（比如 ReLU）、使用更小的学习率，或是更细致的参数初始化方法，以及数据白化（对每层输入做线性变化，如 PCA，调整方差均值，去除特征间关联），都是饮鸩止渴的方法，增大了模型运算量，也容易陷入超参数调整的复杂情境中。
    - BatchNorm
        - 对每一个 batch 进行操作，使得对于这一个 batch 中所有的输入数据，它们的每一个特征都是均值为 0，方差为 1 的分布。单纯限制分布在 $(0,1)$ 之间也不合理，降低了数据的表达能力，因此辅以线性变换，两个可学习超参数。
        - 在训练过程里，我们等一个 batch 的数据都装满之后，再把数据装入模型，做 BN。但是在测试过程中，我们却更希望模型能来一条数据就做一次预测，而不是等到装满一个 batch 再做预测。这有两个实现方法，一个是保留训练模型中，每一组 batch 的每一个特征在每一层的 $μ , \sigma$ ，这样显然消耗过高存储空间。另一个方法是 momentum，也即考虑过去的均值和当前的均值，比例通过超参数 $p$ 控制。
        - 除了解决 ICS 问题外，在规整数据分布后，也可以相应减轻极端值导致的过拟合问题，进而缓解对 dropout 的依赖和使用。
        - 后续的工作中，各个比较实验也证明了，ICS 问题并不一定导致模型表现差，同时 BN 对 ICS 问题的解决能力也是有限的，BN 有效的原因更可能是它使得优化曲线更加平滑。
    - LayerNorm
        - 背景：BN 在长度不一致的情况下存在问题，尤其是在文本问题中，文本中某些位置缺少足够的 batch_size 的数据，计算出的 $μ, \sigma$ 偏差明显。另外，由于测试中需要使用累积经验统计量，所以当测试文本长度超过训练文本时会缺少对应累积统计量。不过这只是理论上的，实际工程中都是多裁少 pad。也正是因为 LN 更适合处理长度不一致的情况，LN 成为了 NLP 问题中的默认 config，相当于有一个约定俗成的感觉。
        - 区别：相比 BN 在特征间进行标准化操作，LN 是在整条数据间进行标准化操作，也就是对于图像问题，BN 在某个特征通道上做各图标准化，LN 则是在一张图片所有通道和像素值范围内做标准化；对于文本问题，BN 是对整个小批量数据的所有序列 token 的所有特征维度标准化，LN 是对单个序列中单个 token 的所有特征维度标准化。
        - 也就是说，对于 LN，各条数据间在进行标准化的时候相互独立，每当来一条数据时，对这条数据的指定范围内单独计算所需统计量即可。
        - Transformer 论文中原始采用的是 Post-LN，而后提出的改进是使用 Pre-LN，二者的差异在 LN 和残差块（addition）的相对位置关系。Pre-LN 的优势是无需做 warmup，所以收敛速度更快，也规避了不必要的超参调整成本引入。具体原因是 Post-LN 在输出层的 gradient norm 较大（紧接在线性变换后，这里的线性变换是为了放大标准化 + 输出层梯度累积效应），往下层走则呈现下降趋势。因此这种情况下如果不使用 warmup 策略，在初期容易引起模型的震荡。然而也有观点表示 Pre-LN 会损失部分特征，因此同样条件下，Post-LN 能够借助更全面的特征信息得到更优结果。
	- [RMS(**R**oot **M**ean **S**quare) Norm](https://zhuanlan.zhihu.com/p/2162179802)
		- 相当于 LayerNorm 的平替，大幅降低计算量，不需要计算均值（去中心化）和标准差（缩放），只需要计算均方根 $RMS(x)=\sqrt{\frac{1}{n}\sum^n_{i=1}x_i^2}$ 来归一化输入，却能保持和 LayerNorm 相当（甚至更好）的稳定性效果。
		- 原论文还提出了 $\rho$RMS Norm，用前 $\rho \%$ 个样本来估计 RMS，进一步减小计算复杂度。
    - Deep Norm
	    - 在 Layer Norm 之前扩大残差连接，也即 $\text{LayerNorm}(x*\alpha + f(x))$。
- 前馈神经网络简单描述，使用了什么激活函数，公式，有何优劣
	- 线性变换（全连接层）→激活函数（非线性）→线性变换（全连接层）。
	- ReLU 函数，优点是简单高效、减少梯度消失、稀疏激活（负数输入）和高效梯度更新（避免了梯度弥散导致的靠近输入更新缓慢），缺点是死亡 ReLU 问题（输入总负，永远无法更新参数）、输出非平衡（较多 0 造成偏置）、梯度爆炸和不平滑性（$x=0$ 处不连续，导数突然跳变）。
- Decoder 阶段的多头自注意力和 Encoder 有什么区别，为什么需要 sequence mask
	- 编码器阶段，输入序列的所有位置都可以彼此相互关注。
	- 解码器阶段，输出序列是逐步生成的，不应该看到未来的词。
- Transformer 的并行化如何体现，Decoder 端可以并行吗
	- Transformer 的并行化主要体现在 self-attention 模块，在 Encoder 端 Transformer 可以并行处理整个序列，并得到整个输入序列经过 Encoder 端的输出，但是 RNN 只能从前到后的执行。Decoder 在训练的时候可以（正确的目标输出序列已经存在），但是推理输出的时候不可以。
- Encoder 是如何和 Decoder 交互的
	- 编码器处理输入序列，生成每个位置的上下文表示。
	- 解码器通过交叉注意力机制访问编码器的上下文表示，结合自回归生成的方式逐步生成输出序列。
	- 编码器为解码器提供了全局信息，帮助解码器在每一步生成输出时参考输入序列中的关键信息，确保生成的输出序列与输入语义相关联。
- [teacher forcing、exposure bias 和 scheduled sampling](https://zhuanlan.zhihu.com/p/93030328)

### Bert

> GPT 采用 Masked-Attention，对模型和训练数据的要求会更高，因为模型能读到的信息只有上文。而采用普通 Attention 的 Bert 在训练阶段就能同时读到上下文。这个性质决定了 GPT 模型越来越大的趋势。
> 但是，长远来看，Masked-Attention 是 push 模型更好理解文字的重要手段，毕竟在现实中，我们更希望培养模型知上文补下文，而不是单纯地做完形填空。

- Bert 本身是两个任务：MLM（完形填空）和 NSP（预测下一句是否紧跟）。
- Bert 和原生 Transformer 的主要区别
    - 双向、同时在 MLM（遮罩语言模型）和 NSP（下一句预测，二元分类两个句子是否真实相连）两个任务上训练。
    - MLM 中，80% 使用 `<mask>`，10% 随机替换为词表中的其它词，10% 不变。主要是真实任务中不会有 `<mask>` 遮罩词，同时也适当引入噪声，增强泛化。
- 为什么会在开头加 `[CLS]`，可以有别的替代方案吗
	- 序列全局语义表示，作为分类任务的输入综合决策，也用于 NSP 任务保持了一致性。可以有别的方案，例如最后一层所有词的 embedding 做池化，但是可能会弱化部分特征信息，尤其是在句子长度较长时。
- [为什么三个 embedding 可以相加（token、segment 和 position）](https://www.zhihu.com/question/374835153/answer/1506279757)
	- 本质上是特征融合，直接相加和 concat 后过全连接层其实是一致的。
- [为什么要用 WordPiece/BPE 这样的 subword Token](https://zhuanlan.zhihu.com/p/460678461)
    - 基于空格的分词器：罕见词难表示，且容易出现大词表问题，token 的 one-hot 向量会很长，增加了 embedding 的训练参数量，并且冗余严重，同义不同形式的词算作不同词（loved，loving）。
    - 基于字符的分词器：单个字符本身缺乏语义，且增加了输入序列长度。
    - 基于子词的分词器：借助基本单元，相当于介于上述二者之间。实现是基于统计学的，不是基于语言学的。
        - Byte Pair Encoding（BPE）：统计连续字符对的出现频率，注意结尾要加上结束字符，相同的连续字符对出现在开头末尾的意义往往是不同的。
        - WordPiece：BPE 关注【频率最高】，WordPiece 关注【概率最大】。WordPiece 在每次合并时将选择一对相邻的子字符，这对子字符满足：当合并它们时，它们对语料库语言模型的概率提升最大。
        - Unigram Language Model (ULM)：BPE 和 WordPiece 没有关注相同单词的不同划分在不同场景下的表现。ULM 首先生成大词表，然后基于这张词表，对所有语料的所有子词划分结果考察，不断丢弃对总体评分贡献较低的子词。
	- 常见的解决 OOV（词表溢出）的方案
		- 切分：将罕见词切分为子词单元（如 BPE），或直接退回到使用 byte 处理（LLaMA，但是又增加了训练和文本生成时间）。
- [Bert 中是如何处理一词多义问题的](https://www.zhihu.com/question/417617174/answer/2364201821)
	- 主要是因为关注了上下文，尤其是在 MLM 任务下，经过 self-attention 操作后，在不同上下文语境中的词向量是不一致的。
- [以 Bert 为例，计算模型参数量](https://snailcoder.github.io/2023/07/05/how-many-parameters-in-bert.html)
- Bert 为什么要使用 warmup 的学习率 trick
	- 避免梯度爆炸，同时也避免了部分注意力头因初始化不佳或学习率波动而出现训练不稳定的情况。
- 为什么说 GPT 是单向的而 Bert 是双向的，对比 NTP 和 MLM 任务
	- 前者只使用左边的上文，后者使用左右的上下文，所以 GPT 更适合续写生成类任务，在处理多义词和全局信息理解时可能遇到困难。
- prefix LM 和 causal LM 区别是什么？
    - Prefix LM（前缀语言模型）是一种生成模型，在生成时，前缀语言模型会根据给定的前缀（即部分文本序列，可以是上文，但不一定是上文）预测下一个可能的词。这种模型可以用于文本生成、机器翻译等任务，比如 GPT。
    - Causal LM（因果语言模型）是一种自回归模型，它只能根据之前的文本生成后续的文本，而不能根据后续的文本生成之前的文本。在训练时，因果语言模型的目标是预测下一个词的概率，给定之前的所有词作为上下文。这种模型可以用于多轮对话、语言建模等任务，比如 ChatGLM。
- [Bert 存在的问题](https://zhuanlan.zhihu.com/p/347846720)
	- MLM 对单个 token 随机 mask，丢失了短语和实体信息，在中文语料上尤其明显。
	- MLM 仅预测被 mask 的 token，其余 token 没有参与预测，而 GPT 等模型可以对每个 token 进行预测，导致 MLM 训练效率偏低，训练时间过长。
	- NSP 任务可以学到句子维度的信息，但是仅为二分类，且负样本构造简单（随机采的，不一定来自同一主题），导致模型不能充分训练，消融实验显示 NSP 对下游任务作用小于 MLM。
- ALBert（A Lite Bert）
	- 对嵌入参数化进行因式分解，大的词汇嵌入矩阵分解为两个小的矩阵，先映射到低维词嵌入空间，再映射到隐藏空间。另外通过跨层参数共享进一步提升参数效率。
	- 提出 Sentence-order prediction (SOP) 来取代 NSP，负例是选择一篇文档中的两个连续的句子并将它们的顺序交换，这样两个句子是同主题下的，能够学到连贯性信息。
- RoBERTa
	- 采用更大的参数量、数据集和 batch size。
	- 动态掩码，每次输入一个序列都会生成新的掩码模式。
	- 使用字节级 BPE 的文本编码来处理数据，原生 bert 使用粒度较大的字符级 BPE，减小稀有字符 OOV（不在词表）问题。
	- 改进输入格式并取消下一句预测任务，连续抽取句子直至塞满长度限制（512），限制不能跨文档抽取在实验中获得了更好的效果。

### GPT

- GPT3 和 GPT2 的区别，发展史，InstrutGPT 解决对齐问题
    - [Codex](https://zhuanlan.zhihu.com/p/611313567)：GPT3 架构上，预训练 + 微调，微调数据选取算法网站和 Github 上使用 CI 的函数，包含单元测试，后续可用于评估代码是否准确。输出采用 Nucleus Sampling，取词至总和不小于 0.95，然后采样（随机性 + 确保不采到概率太低的词）。评价使用 $pass$@$k$ ，也即从模型的答案中随机抽取 k 个后，能从这 k 个里得到正确答案的概率。

### [LLaMA](https://zhuanlan.zhihu.com/p/696571171)

- 基本结构
	- Transformer Decoder
	- PreNorm + RMSNorm
	- SwiGLU(SiLU) 激活函数，公式是 $\sigma(x) \times x$，效果类似平滑版的 ReLU，能够在取得较低困惑度方面发挥更好的效果。
	- RoPE 位置编码。
	- BPE 分词算法，数字 digit 单独拆分，未知的字符拆分到 byte 级别，进而能够通过 byte 的方式构造出不在 vocab 的字符，词表大小 32k。
	- AdamW 优化器。
- [LLaMA 2.0](https://zhuanlan.zhihu.com/p/653303123)
	- 训练数据集增加约 40%。
	- 上下文长度从 2048 到 4096。
	- 7B 和 13B 使用与 LLaMA 相同的架构，34B 和 70B 模型采用分组查询注意力（GQA）。
	- RLHF 发挥了显著的作用，二元比较标注样本（偏好差距分为四类），主要关注模型的有用性（helpfulness，如何回应以满足用户的请求和提供所需的信息）和安全性。除此之外还探索了拒绝采样（RS）和 PPO 两种算法。
	- Code LLaMA：以 LLaMA 2.0 为起点微调。
- [LLaMA 3.0](https://www.breezedeus.com/article/llama3)
	- 数据集大幅增加，代码数据和非英语数据增加，微调数据集（指令/人工注释）中还是英文为主，所以迁移到中文任务上需要中文数据集微调。
	- 流水线过滤：启发式过滤器、NSFW 过滤器、语义去重方法和文本质量分类器等。
	- 所有参数量的模型都使用了 GQA。
	- tokenizer 由 sentence piece 更换为 tiktoken，词表从 32k 到 128k，更高效编码文本，但也导致嵌入层输入输出矩阵增大。
	- 输入输出长度从 4096 增加到 8192。
	- 结合了模型并行、数据并行和管道并行。
	- 对齐使用了 DPO。
	- 合成数据，例如代码数据中使用了执行反馈、编程语言翻译和文档反向翻译回代码。
- [LLaMA 3.1](https://www.breezedeus.com/article/llama3.1)
	- 405B 模型，旗舰模型的参数规模预测
	- 确定数据混合比例：知识分类（便于对网络上过度代表的数据类别进行降采样，例如艺术和娱乐） + 缩放法则预测混合比例。预测确定通用 50%，数学和推理 25%，代码 17%，剩下 8% 是多语言，最终在预训练中增加了非英文比例，上采样了推理 token。
	- 工具调用，和 OpenAI GPT 类似。
- LLaMA 3.2
	- 小中型视觉 LLM（11B、90B）和端侧轻量级纯文本 LLM（1B、3B），后者支持 128K 上下文。
	- 重点强调了安全和隐私保障。
- ？LLaMA 4
	- 预计更关注 Agent。

## 大模型炼丹

> [!summary] 以下问题提纲挈领为主，考察更多是结合任务场景，需要联系实践，大多无标准答案，主要靠积累。

### 基础理解

- 下一个词序列预测：其实可以被看做是包含语法解析、语义理解和风格模仿的多任务学习。
- [过去三个月，LLaMA 系模型发展如何？指令微调的核心问题又是什么？](https://mp.weixin.qq.com/s/cXPNyOeK9vFjJcgxc_LqZQ)
- LLM 的训练目标
    - 最大似然目标，也就是最大化模型生成训练数据中观察到的文本序列的概率。具体来说，对于每个文本序列，模型根据前面的上下文生成下一个词的条件概率分布，并通过最大化生成的词序列的概率，使用梯度下降法等优化算法来优化模型参数。
- [涌现能力及其理解](https://zhuanlan.zhihu.com/p/621438653)
    - 两类任务： In Context Learning(Few Shot Prompt) 和思维链 (CoT)
- [为什么现在的大模型大部分是 Decoder only 结构](https://www.zhihu.com/question/588325646/answer/3357252612)
    - 泛化性能（zero shot、few shot）
    - 效率问题，复用 KV-Cache，多轮对话友好
        - [KV-Cache](https://zhuanlan.zhihu.com/p/686183300)：保存了 K 和 V 矩阵投影和每层向量的乘积结果，在计算注意力时空间换时间。成立的条件是每一个 token 的输出只依赖于它自己以及之前的输入，与之后的输入无关。Bert 就不符合这个条件，另外对位置编码有特别设计，每次增加新的 token 后会改变同一 token 位置编码的 LLM 也不符合。
    - 先发优势
    - 多样化解码生成
    - 讨论其它架构的模型：BERT、T5、BART、UNILM

### [解码策略](https://www.zhihu.com/tardis/bd/art/647813179?source_id=1001)

- 贪婪解码：直接选择概率最高的单词，最简单的方案，但是容易导致重复现象更加明显。
- Beam Search：维护一个大小为 k 的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k 个单词，然后保留总概率最高的 k 个候选序列。这种方法可以平衡生成的质量和多样性，但是计算资源消耗大（长序列下甚至很容易出现组合爆炸）、k 的选择问题需要实验调参，并且可能陷入局部最优解。
- 随机采样：按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义。
- Top-k 采样：优化自贪婪解码，每次从概率最高的 k 个 token 中随机选择，也就是说 k=1 时退化为贪婪解码，缺点是同样没有考虑单词之间的语义和语法关系，以及 k 的取值难以确定，在某些极端的概率分布下可能采样到概率很低的 token。
- Top-p 采样：在每一步，只从累积概率超过某个阈值 p 的最小单词集合中进行随机采样，而不考虑其他低概率的单词，可以在没有概率低的 token 下也保留一定的多样性。top-k 和 top-p 可以同时使用，后起作用（覆盖）的是 top-p。
- Temperature 温度：来源于统计热力学，高温意味着更可能遇到低能态（logits），具体实现是在 `softmax` 的结果上除以温度。越低的温度使模型对其首选越有信心，而高于 1 的温度会降低信心，0 温度相当于贪婪解码，而无限温度相当于均匀采样。如果想进行贪婪解码/beam search 生成确定性结果，可以设置 `do_sample=False`，不必设置温度。但是如果使用 `do_sample=True` 选项，并且仍然希望生成结果几乎确定性，温度一般设置为非常小的值（严格浮点数，避免除以零），例如 1e-3 甚至 1e-8。
- 如果 top-k、top-p 和温度联合使用，起作用的顺序依次是 top-k→top-p→温度。
- repetition_penalty 惩罚重复：在每步时对之前出现过的词的概率做出惩罚，即降低出现过的字的采样概率，让模型趋向于选择解码为没出现过的词。
- no_repeat_ngram_size：限制 n-gram 在结果中出现次数。
- 对比解码：整体思想是通过查找到最大化强模型和弱模型之间可能性差异的字符串来生成文本。

### 常见问题

- 复读机问题
    - 数据偏差：数据集本身重复度高，多样性不足。
    - 训练目标：自监督学习可能使得模型更倾向于生成与输入相似的文本。
    - 解码方式：贪婪搜索中 beam 和采样中温度的设置。
    - [Self-Reinforce](https://zhihu.com/question/616130636/answer/3166309896)
- 长文本处理
    - 分块或分层（段落、句子等）处理，可适当引入重复以确保连贯性。
- 领域微调后，通用能力遗忘问题
    - 数据方面：保留一定比例的通用数据。
    - 训练方面：领域自适应、对抗学习、强化学习、多任务学习和增量学习（交替使用通用数据集和领域数据集，或逐步引入）。
- 数据处理
	- 不同任务场景下的数据集格式
	- 筛选指令微调数据
		- [IFD 指标](https://zhuanlan.zhihu.com/p/658128530)：少量数据进行模型初学习（有的 base 模型无指令遵循能力），聚类后抽取样本确保多样性。然后利用训练好的模型计算所有数据的 IFD 指标，具体来说就是有指令的交叉熵和无指令的交叉熵的比值。较高的 IFD 分数表明模型无法将答案与给定的指令内容进行对齐，表明指令的难度更高，更需要模型训练中优化。
- [LLM 不擅长准确数学运算的原因](https://www.zhihu.com/question/654932431/answer/3522356508)
	- 早期 LLM tokenizer 的分词问题：每个数字需要被单独切分成一个 Token（LLama-1），这是能够做好数学运算的必要条件，但不充分。
	- 数字序列输入顺序：逆序输入符合人类计算方式，同时也适配 Next Token Prediction 这种输出方式，正序计算还需要存储高位临时结果。
	- 数字对齐问题：两种解决方案，一种是加入位置提示，相同位置使用相同提示符；另一种是对每个数字 Chunk 单独引入新的位置编码。
	- 长度外推问题：训练时只见过 10 位加法很难完成 20 位的，这个问题主要靠位置编码解决，FIRE（Functional Interpolation for Relative Position Embeddings）目前外推能力最强。
	- 幻觉，或者说本质上还是训练数据不足。
- [幻觉问题](https://www.zhihu.com/question/635776684)
    - 什么时候容易产生幻觉
        - 数值混淆，尤其是涉及数字和日期时。
        - 长文本处理，长期依赖关系中可能存在自相矛盾内容。
        - 逻辑推断障碍，模型错误解读了源文本中信息。
        - 上下文与内置知识的冲突，比如领域知识和预训练知识中相同词语的不同意义。
        - 错误的上下文知识，尤其是上下文包含错误假设时，模型可能难以识别。
    - 解决：从数据、训练和推理三个角度讨论。
    - [模型遗忘](https://zhuanlan.zhihu.com/p/665691828)：用于版权、隐私保护和偏见信息处理。模型的遗忘不是完全对目标信息返回为空，而是出现幻觉，比如哈利波特的角色信息被遗忘，返回他是一个导演作家等。
        - 实际策略：先对要遗忘的信息构建特定数据集，使其对该文本的预测更加倾向于原始内容。也就是改变最开始模型的相关标记，从而方便下一步，使用模型自身的预测能力，为每个标记生成替代标签。
- 节省显存
    - 存储分类
        - 模型必要相关的：模型梯度、模型参数和优化器状态。
        - 非模型必须的：激活值（反向传播中计算梯度更快）、临时存储（例如把梯度发送到某 GPU 总聚合）、碎片化存储空间。
    - 梯度累积 Gradient Accumulation
    - 网络剪枝
    - 量化方法：FP32、FP16 和 BF16。
        - fp16 中 5 位用来表示指数位（除去全 0 和全 1，一共是 $2^5-2=30$，也就是可以表示 $[-14,+15]$），10 位用来表示小数位，剩下 1 位是符号位。其中指数位全 0 表示非规格数（+0、-0 和极其接近 0 的数字），全 1 表示特殊数（小数位全 0 表示 Inf，符号位确定 +Inf 和 -Inf，小数位不全为 0 表示 NaN），所以 fp16 的最大绝对值是 $(1+\frac{1023}{1024})*2^{15}=65504$，而最小正值是 $\frac{1}{1024} *2^{-14}=2^{-24}=5.96E-8$。
        - fp32 和 fp16 基本一致，符号位 1 位，指数位 8 位，尾数位 23 位，因此 fp32 的动态范围为 $(1.4E-45$ ~ $3.40E38)$。
        - bf16 则是保留了和 fp32 一样的指数位，也就是在尾数位做了截断，只剩下 7 位尾数位，因此就是牺牲了精度来换取几乎和 fp32 一样大的取值范围，避免 fp16 的溢出问题。bf16 的最大绝对值是 $(1+\frac{127}{128})*2^{127}$，而最小正值是 $\frac{1}{128} *2^{-126}=2^{-133}$。bf16 的动态范围为 $(9.2E-41$ ~ $3.38E38)$。
        - 为什么要使用半精度
            - 占用内存更少，可以选用更大的 batch_size。
            - 训练时通信量大幅降低（尤其是多机多卡），加快数据流通，大幅降低等待时间。
        - 混合精度训练
            - 不能全部替换为半精度：**溢出问题**（超过正负最大值）和**舍入误差**（比最小正值小的加减被舍去）
            - 权重的高精度备份：使用 fp32 权重作为精确的 “主权重 (master weight)”进行备份，而其他所有值（weights，activations， gradients）均使用 fp16 进行计算以提高训练速度，最后在梯度更新阶段再使用半精度的梯度更新单精度的主权重，这样当很小的梯度乘上学习率后要跟权重（fp32 的）做运算时，就不会被舍弃了。
            - Loss Scale：大部分的梯度值都很小，因此该方法通过让梯度乘一个 scale，从而整个分布右移、占据更多 fp16 可表示的范围。
            - 算数精度：神经网络主要涉及三种计算，向量点乘（常见于全连接层），归约（Reduction，减少张量元素，常见于归一化和池化层等），点运算（pointwise，常见于 ReLU、tanh 等激活函数），其中向量点乘中加法使用 fp32 完成，存储结果则使用半精度，reduction 也要用 fp32 来做，但以半精度方式存储；而 pointwise 的运算主要受到 memory-bandwidth 限制（计算简单且可以良好并行化），因此它们是以单精度还是半精度运算，都不影响计算速度，所以单、半精度均可。
        - 由于 fp16 的范围有限，因此在训练中很容易出现溢出问题，因此一般**在训练时都使用 bf16**，若要使用 fp16 则通常搭配 loss scale 操作。而 bf16 能够和 fp32 有相近的范围，且由于表示范围略小于 fp32，可以起到**隐式正则化**的作用，避免过拟合（训练时参数通常呈现幂律分布，在 bf16 中,outlier 值由于超出了表示范围而被 clipped，且大部分值仍在范围内，不会明显降低模型性能）。而在推理时，由于 fp16 精度更高，参数范围也通常趋于稳定，所以通常都直接使用 fp16。
        - 由于额外增加的反量化和重量化操作的存在，推理和训练一般都不会节省时间，反而会增加时间，因此也勉强可以算是一个时间换空间的例子。
        - 半精度仍然不够的情况下，也可以进行 8bit 或 4bit 量化。
        - TF32：由 1 个符号位，8 位指数位（对齐 FP32）和 10 位小数位（对齐 FP16）组成，实际只有 19 位。在性能、范围和精度上实现了平衡，用于替代 FP32。
        - NF4：将浮点权重在量化的同时归一化到以 0 为均值、标准差在 $[-1,1]$ 范围内的正态分布上。首先将浮点权重参数离散化为 4 位整数值，然后计算正态分布固定期望值，最后标准化，于 QLoRA 首次使用。
    - 知识蒸馏：大型预训练模型上进行推理，并使用其输出作为目标标签，来训练一个较小的模型。
    - 加速相关
	    - Flash Attention：参考前文
	    - vllm

### 微调相关

- 预训练和微调是哪个阶段注入知识的
    - 知识注入是在预训练阶段进行的，预训练模型通过大规模通用数据的训练，学习到了丰富的语言知识和表示能力。
    - 微调的目标是将预训练模型中学到的通用知识和能力迁移到特定任务上，提升模型在目标任务上的性能。
- 微调是啥
    - 冻结底层权重（通常卷积层） + 替换顶层分类器（通常全连接层） + 解冻部分权重（可选）
- [提示微调](https://zhuanlan.zhihu.com/p/635686756)
    - BitFit
    - Prefix Tuning
    - Prompt Tuning
    - [P-Tuning](https://zhuanlan.zhihu.com/p/635848732)
    - P-Tuning v2
- 适配器微调
    - [Adapter Tuning](https://zhuanlan.zhihu.com/p/636038478)
    - AdapterFusion
    - AdapterDrop
    - [MAM Adapter](https://zhuanlan.zhihu.com/p/636362246)
    - UniPELT
- [LoRA](https://zhuanlan.zhihu.com/p/636215898)
    - 主要思想：冻结预训练模型的参数，并选择用 A 和 B 矩阵来代替，在微调下游任务的时候，只更新 A 和 B。
    - 优势：节省资源、共享模块（替换 AB 矩阵快速切换下游任务，即**可插拔**）、不会引入推理延迟（全量微调后需要重新读写内存、缓存失效、针对参数优化失效）、和其它许多方法正交（如 Prefix Tuning）。
    - 基础模型对 LoRA 训练影响较大，增大数据量和参数量可以提升效果，但是并不是仅通过提升数据量和参数量就可以让本来学习力更低的模型效果超越学习力更强的模型。
    - [LoRA](https://zhuanlan.zhihu.com/p/646831196)
        - 对 A 矩阵随机高斯初始化，对 B 矩阵零初始化，保证**训练开始时旁路为 0 矩阵**，不会在开始引入噪声，理论上二者初始化情况也可以交换，并没有明显差异。
        - 另外有缩放因子 $\frac{\alpha}{r}$，其中秩 $r$ 就是信息量的表现形式。已知 SVD（奇异值分解）能够关注到最强调的几项特征，而 $W$ 代表旧知识，对其作 SVD 没有意义，$\Delta W$ 代表新知识，但是该项并不是确定的，只有全参微调后才能确定。因此最终只能设置**秩为超参**，让模型自行学习低秩矩阵 A 和 B。除此之外， $r=64$ 的前八个特征并不一定和 $r=8$ 完全相同，模型会尽可能往信息最丰富的维度学，但不一定学出来的就是真实的 top $r$ 。因此我们在模型的不同部分，比如 $W_q$ 和 $W_v$ ，也可能采用不同的秩。
        - $\alpha$ 则一般设置为第一次实验时的 $r$ ，第一次的 $r$ 通常会设置较大，从而尽可能更近似 $\Delta W$，此时缩放因子为 1，意味着假定 LoRA 微调效果和全参微调持平。而后自然会逐渐减小 $r$ ，此时缩放因子随之增大，也就是保持新知识对模型权重的影响。除此之外， $r$ 较小意味着精炼但不全面，梯度下降方向更加确定，可以适当放大影响；而 $r$ 较大意味着全面而有冗余或噪声，适当减小步伐也是合理的。
        - 正常情况下，输入的信息因为注意力机制会关注重要信息和省略无意义信息而导致信息冗余，体现在矩阵上就是**不满秩**，因此使用这种方式可以大幅降低参数计算量。同时由于噪声的影响，可能在某些场景下效果甚至能超过全量微调。
    - [AdaLoRA](https://zhuanlan.zhihu.com/p/657130029)
        - 主要思想：LoRA 中对不同模块使用相同的秩，且秩设置不变均不合理，所以总体目标就是**动态调整不同模块下的秩**。微调高层参数（比如 FFN）的效果会比微调底层参数（比如 attention）的效果更好。
        - LoRA 使用了 $\Delta W=U\Sigma V^T\approx BA$ 近似，而 AdaLoRA 则直接使用三个矩阵 $P, \Lambda, Q$ 去近似，其中中间 $\Lambda$ 矩阵是对角矩阵，初始化为 0，另外二者随机高斯初始化，原因同样是保证训练开始时无噪声。
        - 实际计算中， $\{P_{*,i}, \sigma_{i}, Q_{i,*}\}$ 组成三元组，$P$ 表示第 $i$ 列，$Q$ 表示第 $i$ 行，根据重要性分数，将不重要的三元组中的 $\sigma$ 置为 0，相当于 mask，**不是直接删除**的原因是，模型学习是探索的过程，在一开始模型认为不重要的三元组，在后续过程中模型可能会慢慢学到它的重要性。能够保留奇异向量，也是 AdaLoRA 表现优于 LoRA 的一个原因。
        - 三元组的重要性分数 = $\Lambda$ 的重要性分数 + $P$ 矩阵中所有元素重要性分数的均值 + $Q$ 矩阵中所有元素重要性分数的均值。取均值的原因，是不希望参数量影响到重要性分数。而单参数的重要性分数则定义为参数权重和损失函数在该参数上的梯度乘积的绝对值，然后加以 momentum 消除不同 batch 上的波动，也就是减轻单个 batch 样本带来的重要性的评估误差，对于这个引入的不确定性 $U$ ，在计算时也考虑平滑后的差异，不可忽略真实的波动情况。
        - 筛选重要三元组的策略称为 top_b，在训练刚开始的过程中逐渐增大 top_b，也就是加秩，让模型充分探索，到后期开始逐渐降低 top_b，最后以相对稳定的 top_b 进行训练，整个过程和 warmup 类似。
        - AdaLoRA 的损失函数由两部分组成，一部分是正常的训练集损失函数，即预测值和真实标签之间的差距，而另一部分是 $P$ 和 $Q$ 和**满足正交矩阵性质**的差异，因为真实的 SVD 中，$U$ 和 $V$ 都是正交矩阵。由于 LoRA 在训练时没有引入任何和 SVD 性质相关的约束，所以往往 AdaLoRA 比之能够具有更好的效果，训练时能够更加稳定，泛化性能更好。
    - [QLoRA](https://zhuanlan.zhihu.com/p/666234324)
        - NF4：四位标准浮点数量化，其中结合了**分位数量化**（分为若干相等块，使用累积分布函数的反函数简化计算）和**分块量化**（将张量分成若干个块，每个块都有独立的量化常数，也就是该块最大值，对其进行归一化，反量化就借助其和存储的量化后的低精度值恢复到高精度值，分块的优势在于不容易出现极端值，导致整体量化后极大或极小）。关于 0 的处理是正数取 9 个值，负数取 8 个值，均会取到零点，然后去重，也就是确保 0 的映射值是 0，并用满 4 位数据类型的全部 16 位。
        - 双重量化：由上得知，在模型保存时，除了保存量化后的低精度值，也要保存对应的量化常数。而这个常数是 FP32 高精度值，会额外占用较高显存，因此需要对这个常数也做一次 8bit 量化，QLoRA 以每 256 个量化常数为一组。同样，在反量化时，也因此需要两次操作。
        - 分页优化：当显存不足时，将保存的部分梯度检查点转移到 CPU 内存上，和计算机的内存数据转移到硬盘上的常规内存分页类似，牺牲时间换空间，主要是解决显存**峰值占用**问题，论文的创举是在消费级显卡上训练 33B 模型，在此场景下应该是必需的。是对梯度检查点的进一步优化，梯度检查点是在牺牲显存保存前向传播的激活值，和节省显存重新依照损失函数计算激活值之间的权衡。丢失部分激活值，有保存的就用，没有就重新计算。
        - QLoRA 也在原参数一侧添加了一个与原参数并行的低秩适配器，它的精度是 BF16。也就是说，QLoRA 有一个 4NF 的存储数据类型和 16BF 的计算数据类型，在计算前向和反向传播时通过反量化为 BF16 计算。
        - QLoRA 主要关注点都在尽可能节省显存，而在微调训练方面几乎和 LoRA 一致。

### RAG 检索增强生成

- 解决问题
    - 长尾知识：相对通用和大众的知识结果更准确，剩下的通过增大训练集或者增加参数性价比较低，通过检索知识在上下文给出更加经济。
    - 数据新鲜度：无需重新训练模型就加入更新的知识，因此频繁更新的知识建议单独作为外部知识库。
    - 私有数据：在训练中加入私有数据成本较高，且有隐私信息泄露风险。
    - 来源验证和可解释性：在生成的结果和信息来源之间建立关联，约束生成空间，注意力更加关注知识库中内容，可以缓解幻觉问题。
- RAG vs. SFT
    - 数据方面，RAG 能够确保数据保持最新，在面对频繁变更的数据时成本消耗也偏低。
    - 生成方面，RAG 不容易产生幻觉，但是无法保证形成特定风格的输出，响应阶段相对透明（比如可以提供检索的匹配度）。
    - 实现方面，RAG 关注的核心是检索策略以及数据的存取和更新，SFT 则是数据集的构建、微调目标定义和计算资源准备。
- 索引形式
    - 链式索引
	    - 典型场景是数据源包括专业文献，先根据摘要召回文献合集，然后再检索召回具体的 chunk 片段。
    - 树索引
	    - 需要能够有多级分类的场景，例如首先依据类别关键词仅检索目标类下的内容，然后依据类下的具体标签小类进一步缩减搜索范围。甚至可以将自然语言转换为元数据，然后凡是类似 SQL 中使用 `where` 查询的情况都可以采用类似方式直接基于元数据过滤。
    - 关键词表索引
    - 向量索引
- 查询变换
    - 同义改写/扩展查询：生成多个类似的查询，然后各个问题都去找文档，需要去重，并且可能分散注意力，因此这种方法必须要搭配重排序。
    - 查询分解：将一个查询分解为多个子问题。
    - HyDE：假设文档回复，也就是先生成一个答案，根据这个问答去查询，可能产生幻觉。
- 检索和重排序：初检索注重效率，选择出 TOPK 召回，然后由重排序（比如时间/时效性）进行精确比对。需要重排序：将一个文档变为向量后势必损失一些信息。另外，重排序后也可以保留尽量少但是高相关的文本，减小上下文长度提升性能。
	- Bi Encoder：每个输入文本独立编码，嵌入向量可以预先计算和缓存，适合高效匹配大规模语料库场景，因此被用于 Retriever。
	- Cross Encoder：将两个文本拼接作为联合输入，通常是以 `[CLS]` Text1 `[SEP]` Text2 的形式输入到一个编码器中，模型可以捕捉到它们之间的精确交互和复杂关系，但是效率低下（每次都需要重新计算无法预存），因此被用于 Reranker。
- Retriever 选择
    - Sparse Retriever：如 BM25，**效率高**尤其是针对大规模文本库；**可解释性强**，词频词匹配等规则帮助用户理解为什么能够检索成功；**部署简单**，所需的计算资源相对较少。
    - Dense Retriever：**语义理解能力强**，能够捕捉查询和文档之间的深层语义关系；可以**处理复杂查询**，尤其是需要上下文理解或多层次语义查询的场景。
    - 中庸的策略是使用 Sparse Retriever 做初步检索，再使用 Dense Retriever 做精排，本质上和搜推的思路类似。
    - 在训练 Dense Retriever 时，使用**对比学习**，构建正负样本对非常必要，尤其是负样本，需要关注**难负样本的质量**，否则模型在检索到无关文本时很可能效果骤降。
    - Learned Sparse Retriever(e.g. BGE-M3)：先通过 BERT 等深度学习模型生成 dense embedding，再引入额外的步骤对以上 dense embedding 进行稀疏化，得到一个 sparse embedding。
	    - BERT 在相似比较时仅关注第一个 `[CLS]`token，而 BGE-M3 扩大到关注序列中所有 token，dense retrieval 仍然使用 `[CLS]`。
	    - 在 encoder 输出层上又增加一个线性层和 ReLU 激活函数，得到每个 token 的权重（有重复出现的 token 取 max），此时就可以使用和 TF-IDF 相似的思想，开展 sparse retrieval，计算两组向量相似度使用的是共现计算。同时 ReLU 的结果是非负的，有助于 embedding 的稀疏性。
	    - 一阶段使用 [RetroMAE](https://zhuanlan.zhihu.com/p/676410726)(Pre-Training **Retr**ieval-**o**riented Language Models Via **M**asked **A**uto-**E**ncoder) 和对比学习，训练 dense 向量。
	    - RetroMAE 是由 bert 的 encoder 和一层改进后的 transformer decoder 组成，将低掩码率的的文本（mask15%~30%）输入到 encoder 中得到 embedding 向量，将该 embedding 向量与高掩码率（mask50%~70%）的文本输入到浅层的 decoder 向量中，输出完整文本。这种预训练方式迫使 encoder 生成表征能力强的 embedding 向量（decoder 阶段足够困难），在表征模型中提升效果显著。
	    - 除此之外，RetroMAE 论文认为先前的 decoder 学到的信息不足，因此在 decoder 中，将 encoder 获得的 embedding 重复 $N$ 份加上位置编码得到输入 $H_1$，将句子 embedding token 和输入序列（无 mask）拼接并加上位置编码得到 $H_2$，最终 $Q$ 使用 $H_1$，$K$ 和 $V$ 使用 $H_2$。对于 mask，每个 token 能看到的 token 都是通过采样看到的，但是看不到自己，且都能看到首个 token（encoder 产生的 sentence-level embedding 信息）。
	    - 二阶段使用自蒸馏学习（教师模型和学生模型相同），奖励由 dense score、sparse score 和 multi-vector score 三部分组成（1:0.3:1），也就是说模型是主要参考 dense 和 multi-vector，优化 sparse 的。
- 调优痛点
    - chunk_size（块大小）和 similarity_top_k 的超参数选择。
    - 重排序，一方面解决相关性问题（确保找到相关的文档，而不是只包含关键词的文档），另一方面缓解中间丢失问题（模型注意力基本集中于开头和结尾的信息）。
    - 检索策略选择
        - Basic retrieval from each index：基础检索，向量/关键词/结构化字段值。
        - Advanced retrieval and search：高级检索，查询权重/句子级别/最大边际相关（考虑结果多样性和查询相关性）。
        - Auto-Retrieval：自动检索，自动选择合适的策略和参数。
        - Knowledge Graph Retrievers：知识图谱检索，适用于百科等场景。
        - Composed/Hierarchical Retrievers：复合/层次检索，比如 BM25-SVM 复合检索器。融合检索常常能取得不错的结果，但是对效率有所损耗。
    - 格式解析，尤其是在需要特定格式输出的场景。
    - LLamaIndex 支持并行化处理，尤其是在系统处理大量数据的场景。
    - LLamaIndex[切分代码](https://docs.sweep.dev/blogs/chunking-2m-files)：基于 Tree-Sitter 维护的具体语法树（Concrete Syntax Tree， CST）解析工具，速度很快，相比直接切分更关注了语法结构。缺点是依旧是基于行数控制的，可能超过嵌入模型的上下文长度；另外 Tree-Sitter 社区维护的 CST 不能处理所有情况，在文件语言和解析器语言不对应的时候不会有报错信息提示；类信息可能和函数信息相隔较远，需要单独加入。有论文的策略是直接按照空行来切（模拟人类编程习惯，一块逻辑的代码写完空一行）。基于模型的方法也不是不可以，但是整个从数据集到训练到评估的过程非常繁杂，容易有过度设计的感觉，并且也影响效率。
    - 结构化数据和 PDF 等文件处理，采用符号推理和文本推理结合（多数投票机制）。另外，也可以尝试使用 pdf2htmllex 将 PDF 转换为 HTML。
    - Embedding、Retriever 和 Reranker 微调主要是为了提高准确率，而 generator 微调主要为了让模型能够回答【不知道】、格式输出和防止生成不安全的回答。
    - 安全性，对抗提示注入，处理不安全的输出，防止敏感信息的泄露，尤其是在 toC 有未成年用户场景，相关工具有 LLama Guard。
    - 过滤与压缩：上下文中的无关信息可能会对回答质量产生干扰，也浪费算力。有钱不急就使用 LLM 过滤，贫穷快速就选 Embedding 计算相似过滤。
    - 时间加权：一般是两种可能，一个是类似 LRU，热点数据在 FAQ 客服场景中应该搭配更高的权重；另一个是在类似新闻检索场景，越新的数据应该获得更高的权重。
- Contextual Retrieval 上下文检索：Anthropic 提出，给每个 chunk 预先添加上下文，最简单的理解是能够解决单独 chunk 语义不明或歧义的情况，需要搭配 prompt caching 使用（先前已经被用于 system prompt）。
- Graph RAG
- 多模态 RAG

### [向量数据库](https://guangzhengli.com/blog/zh/vector-database/)

> [!summary] 直观上很多想法和理解其实和数据库是相似的。

- 存在优势
	- 去除无关上下文信息以及仅保留多轮对话中相关的部分，节省算力和有限上下文 Token 长度的同时也可能可以提高模型的输出质量。
	- 非结构化数据的聚合存储，例如语音、图片、视频（帧）、地理信息等均可以嵌入向量后存入向量数据库。
- 常见的相似计算方法
	- 欧氏距离：能够反映向量的绝对距离，适用于考虑向量长度的相似性计算，例如推荐系统需要考虑用户历史行为数量，而不能仅靠相似度。
	- 余弦相似度：长度不敏感，只关注向量方向，适用于高维向量相似性计算，例如语义搜索，但如果希望生成尽可能简洁的表示则需要考虑长度特征。
	- 点积：简单高效，适用场景丰富，但在高维场景可能会因为长度敏感存在问题（高维空间长度放大效应）。
	- 另外，如果使用了 padding 操作，也可能干扰相似度计算。而使用平均池化则更容易损失特征信息，最大池化相对平均池化损失特征更少。
- 提高效率
	- 减小向量大小，通过降维或减少表示向量值的长度。
	- 缩小搜索范围，仅检索最接近的簇，主要使用聚类实现。
		- K-Means 和 Faiss 算法：搜索最近的指定个质心下的区域，本质上搜索效率和搜索范围（质量）之间的权衡，这类方法都被归为近似最相邻（Approximate Nearest Neighbor, ANN）。
		- 乘积量化（Product Quantization，PQ）：维护聚类中心索引（浮点数坐标）消耗巨大的内存，尤其是在高维坐标系中还可能遇到维度灾难。解决方案是将向量分解为多个子向量，然后对每个子向量独立进行量化，在子向量维度上聚类，损失一定的精确度，但是节省内存和时间开销。
		- 分层索引（Hierarchical Navigable Small Worlds，HNSW）：本质上和跳表类似，高层索引步长大，用于快速搜索，底层索引步长小，用于准确搜索。缺点是图结构带来的显著存储和维护开销。
		- 局部敏感哈希（Locality Sensitive Hashing，LSH）：设计哈希函数，发生碰撞（相似）的向量被分到同一个桶中，搜索时优先计算哈希后再暴力检索最相似的向量。
		- 随机投影（Random Projection for LSH）：高维场景下数据会随着距离的指数级增长而更加稀疏，LSH 下最极端的情况是每个桶中就一个向量。解决方案本质上也是降维的思想，使用随机投影矩阵将向量和查询都降维到低维空间。生成合适的随机投影矩阵需要较高的计算成本，而投影矩阵的质量（更需要随机性更大的确保信息均匀扩散、泛化、相对距离保留、减少特征间相关性影响和避免过拟合，高随机也可以认为是充当噪声）将显著影响搜索质量。
	- 过滤的时机选择：一般维护有向量索引和元数据索引，前者主要辅助查找，后者便于在特定业务场景下执行过滤。
		- Pre-filtering：在搜索之前过滤，帮助减小搜索空间，但很容易漏掉与元数据筛选标准不符合但是相关的结果。【准确率换效率】
		- Post-filtering：在搜索之后过滤，确保考虑所有相关结果，但是完整搜索后再加上执行筛选更加消耗时间。【效率换准确率】
- 选型考虑
	- 分布式部署：向量数据库使用场景一般有大规模数据，需要确保高可用性和容错性，以及节点数据一致性。想到了分布式事务里的 CAP，一致性、可用性和分区容忍性只能三选二，现有的 NoSQL 主流都是放弃了一致性。
	- 访问控制和备份：经典需求。
	- API&SDK：向量数据库在 LLM 后迅速崛起，API&SDK 规范既不能太复杂，也不能在更新中频繁变更。
	- 从传统数据库扩展：Redis 和 PostgreSQL（pgvector）都有相应的解决方案，主要优势是减轻开发者迁移维护成本，同时可以直接利用如 PostgreSQL 提供的 ACID、并发事务、备份恢复等功能支持。
- Milvus 基础
- 实践中不选择向量数据库的原因
	- BM25 是基于词频逆文档频率（TF-IDF）思想的检索方法，对于关键词匹配和高频短文本的情况有很好的效果，而代码场景正是其中一例（有明确的语法结构）。
	- 向量数据库资源消耗更高，并且需要更积极的监控和维护，而其它的如 BM25 已经是成熟稳定的方案。
	- 向量数据库常用的搜索方案主要都针对字符级，也就是尽可能找长得像的内容，但是很多时候相关内容并不一定字符级相似（这也是为什么需要引入 reranker 来实现相关性排序），而一些轻量级的方案更便于插入或实现自定义规则，在特定任务下更具有针对性。
	- 嵌入模型的选择会极大地影响搜索质量。

### 思维链

- [Test-Time Scaling Law](https://zhuanlan.zhihu.com/p/773907223)
	- 优化推理输入：Prompt。
	- 优化推理输出
		- base generator + Inference: 使用 verifier 的评估结果来指引模型做生成，需要在搜索前通过 prompt 诱导模型按格式产出内容后再搜索。
		- base generator + formatted post training + Inference: 模型从“只产生结果”变成“同时产生中间步骤和结果”，你既可以关注格式遵循，也可以关注中间结果质量。
		- base generator + formatted post training(with inference filtering method) + Inference (selective)：筛选高质量自生产数据做对齐。
	- 对于特别困难问题，在于提升 pretrain 阶段知识注入，Test-Time Scaling Law 作用不大。
	- SFT 数据设置：训练数据是“问题 + 若干（相似）错误 attempts + 正确 attempt”的形式，这一步是让模型模拟人类思考的模式，从步步错误的 attempts 中推出正确的。
	- 即使是 SFT 模型，依然会配合“verifier + 搜索方法”的方式做推理，因为不能确保一条 attempt 链一定有正确答案，也不能保证最后修正的 attempt 一定是正确的，甚至可能出现中间正确修错的情况。
- [使用MCTS增强推理能力](https://zhuanlan.zhihu.com/p/864190605)
	- 搜索树构建
		- 步步推理，每一步有中间结果，最后一步得到最终答案。
		- 一次性推理完毕，相当于上面的方案一次走完得到结果。
		- 拆分原始问题为若干子问题并做相关回答，最后一个子问题的答案就是最终答案。
		- 采用第二种方案的模板，重新回答第三种方案中的子问题。
		- 重新复述原始问题/子问题，例如去掉文字描述，变成形如 condition1…，condition2…的简单格式。

### 强化学习 RLHF

- 出发点：人类偏好对齐。
- [Proximal Policy Optimization（PPO)](https://zhuanlan.zhihu.com/p/677607581)
- 基本思想：智能体观察到环境状态 $S_t$ 和奖励 $R_t$，输出动作 $A_t$，$A_t$ 进一步触发环境的变化 $S_{t+1}$ 和新的奖励 $R_{t+1}$。目标是确定一个策略，能够根据当前观测到的环境状态和奖励反馈，来选择最佳的动作。
- 和 NLP 的契合点：token 是一个个被输出的，因此正好对应了输出动作，而产出的新 token 对应即时奖励，上文 → 上文 + 新 token 对应了状态的转换。实际并不是每个 token 都去更新一次参数，而是有足够的观测数据后。
- 四个基本模型
    - Actor Model：演员模型，对应想要训练的目标语言模型，需要训练。
        - Loss 计算：$actor\_loss=-Adv_tlogP(A_t|S_t)$，其中 $Adv_t$ 表示优势，优势大于 0 那么减小 loss 就需要增大概率，反之优势小于 0 就需要减小状态 $S_t$ 下执行 $A_t$ 的概率。而优势定义如下：$Adv_t=(R_t+\gamma*V_{t+1}-V_t)+\gamma* \lambda *Adv_{t+1}$。由于最后一个时刻优势为 0，那么从后向前计算即可。
        - 为了重复利用 1 个 batch 的数据来做 $ppo\_epochs$ 次模型更新，实际使用 $\frac{P(A_t|S_t)}{P*{old}(A_t|S_t)}$ 用于计算。
    - Critic Model：评论家模型，预估总收益（包含即时和未来），由于未来收益不可知仅能预测，因此也需要训练，最简单的初始化方式是采用 Reward Model。
    - Reward Model：奖励模型，计算即时奖励，参数冻结。
    - Reference Model：参考模型，基于 KL 散度防止模型效果相对 SFT 差异过大，参数冻结。
- 改进：[Direct Preference Optimization（DPO）](https://zhuanlan.zhihu.com/p/721073733)
	- 不再训练奖励模型，直接使用标注的人类偏好数据，一步到位训练对齐模型。
	- 不再使用强化学习的方法，简化对齐优化目标，最后采用类似 SFT 的方式训练对齐模型。
	- 成对回答偏好标注：BT 模型（Bradley-Terry）。
	- 多回答偏好标注：PT 模型（Plackett-Luce），希望最优序列打败其余任何一个可能序列的期望概率尽量大。
	- [泛化能力弱于RLHF](https://zhuanlan.zhihu.com/p/673047773)，更容易受偏好数据质量影响，本质上类似 SFT 会有过拟合风险，导致出现分布外泛化问题，只有离线数据，更加关注数据中标签为正的那部分，相当于只有 0/1 标签表示决策的好坏，而实际奖励数值应该是更加平滑的。
	- 类似问题：[ChatGPT 为什么不用 Reward-Model 的数据直接 fine-tune，而用 RL](https://www.zhihu.com/question/596230048)
		- RL 优化正例和负例之间的差别，而 SFT 可以理解为只有正反馈。
		- 需要划出稳定的边界，让模型能稳定回答它明确知道的东西，让模型不要回答它不知道的/错误的内容，SFT 模型更趋于编造回答，对于模型不会的问题其实在人工标注时应该标注为【不知道】。
		- 过拟合导致的分布外泛化误差。

### 多模态大模型

- CLIP(Contrastive Language-Image Pre-Training)
    - 搜集了大量图像 - 文本对，分别使用 resnet/vit 编码图像、transformer 编码文本，两两计算余弦相似度，只有对角线上的元素对是正样本，其余都是负样本。
    - ViT(Vision Transformer)：将图像拆分为固定大小的 patch，然后对每个 patch 展平为一维向量（通常使用的隐藏层维度是 768），接着加入位置编码（指示每个 patch 在原始图像的位置），最后将包含位置信息的 pathc 向量输入 transformer，结尾一般添加有 `CLS` 分类 token。
    - 对比学习的训练目标（loss）就是最大化正样本的相似度预测，同时最小化负样本的相似度，ITC 损失函数由图像和文本两部分平均，分别是当前图像 - 文本对的余弦相似度相对于总余弦相似度的差距。也就是说是双向的，既要优化从图像预测文本，也要预测从文本预测图像。
    - 实际文本使用“这是一张 `label` 的图片”，能够识别从未在训练数据中出现过的类别，只需要将候选 `label` 中加入可能的未出现过类别。
- ViLT(Vision-and-Language Transformer)
    - 相比起 CLIP 更注重交互而不是嵌入，图像使用 ViT 同时文本词嵌入后都加入模态类型标志，合并成统一序列输入 transformer，使用 preNorm，训练更稳定但可能丢失特征信息。
    - 训练目标是 Image Text Matching（预测图像和文本是否匹配）和 Masked Language Modeling（完形填空任务）。
    - 轻量级、统一使用 transformer 来处理视觉和文本特征，大幅提高了效率，推理很快但是训练较慢。
- ALBEF(Align Before Fuse)
    - 动量蒸馏（MoD)：解决即使是正样本对也存在不相干文本 token 或图像中其它物体的问题，本质上是提高对噪声的鲁棒性。
- BLIP(Bootstrapping Language-Image Pretraining)
    - 由两个单模态编码器（Image Encoder, Text Encoder）、一个以图像为基础的编码器（Image-grounded Text Encoder）和一个以图像为基础的解码器（Image-grounded Text Decoder）组成。
    - 损失函数：ITC、ITM 和 LM。
    - CapFilt（Caption Filtering）机制：处理噪声。
        - Captioner 是 image-grounded text decoder，它在人工标注数据集上以 LM 为目标进行微调，对给定的图像进行文本解码。
        - Filter 是 image-grounded text encoder，它根据 ITC 和 ITM 的目标进行微调，以学习文本是否与图像匹配，去除原始网络文本和合成文本中的噪音文本。
        - Bootstrap 过程，Captioner 生成的图文对与 Filter 过滤后的网络图文，再加上人工标注的图文对结合起来，形成一个新的数据集，重新预训练一个新模型。
    - BLIP2：引入了 Q-Former，负责弥合视觉和语言两种模态的差距，由 Image Transformer 和 Text Transformer 两个子模块构成，它们共享相同自注意力层。
        - 使用 ITC、ITM 和 ITG 预训练对齐，然后冻结的 Image Encoder 生成原始的图像特征，而 query tokens 和 Q-Former 从原始图像特征中生成转化好的图像特征，然后该图像特征经过全连接层映射到 LLMs 的文本 embedding 空间中。然后这些映射后的图像特征，就相当于视觉 prompts，和文本 embedding 一起，输入到冻结的 LLMs 中，最后生成目标文本。
        - Q-Former 作为视觉语义提取器，本身非常难学好。
	- BLIP3

### 分布式并行

- 优化目标：更快地训练更大的模型，难点在于 GPU 的内存限制（更多的参数量、中间结果和训练数据，对应更大模型）和 GPU 间的带宽限制（通信时间，对应更快训练）。
- [数据并行(DP)](https://zhuanlan.zhihu.com/p/617133971)
    - DP：计算 GPU 称为 Worker，梯度聚合 GPU 称为 Server。在实际应用中，为了尽量减少通讯量，一般可选择一个 Worker 同时作为 Server。
        - 存在的问题有两个，一个是每个 GPU 都存了一份模型，冗余严重；另一个是通讯开销大，Server 一般成为瓶颈，针对这个问题可以采用异步的思想解决，但对一个 Worker 来说，只是等于 W 不变，batch 的数量增加了而已，在 SGD 下，会减慢模型的整体收敛速度。
        - 受限于通信负载不均，DP 一般适用于单机多卡场景。
    - DDP：分布式数据并行，首先需要解决通信负载不均的问题，思路是将 Server 的通信负载平摊到每一个 Worker 上，称为 Ring-AllReduce。
        - Reduce-Scatter：每个 GPU 只和相邻两块 GPU 通信，定义一个环状数据通信路线，直至每一块 GPU 上都有一块数据拥有了对应位置完整的聚合。
        - All-Gather：目标是把红色块的数据广播到其余 GPU 对应的位置上，相当于上一步是在数据合并，这一步是直接替换。
        - DP 和 DDP 的总通讯量相同，只是分摊问题导致 DP 需要更多的时间搬运数据。
    - [ZeRO]()：本质上是将能切分的都切分，用较小增加的通讯量换回大量存储量的节省。
	    - ZeRO-DP（针对模型必要或相关存储内容的优化）：模型并行的形式，数据并行的实质。模型并行是针对同样的输入，只使用自己维护的那部分参数来计算结果后整合，而 ZeRO 是在整合权重后，根据不同的输入计算结果后聚合。
	    - ZeRO-R（针对其余存储内容的优化，需要更灵活的手段）：针对激活值，同样采用切分并且可以灵活选用保存和重新计算的比例。针对碎片空间，设置机制适时整合存储空间。针对临时存储，设置固定大小的内存 Buffer，可以提高带宽利用率（GPU 数量上升切片会减小，可以积攒数据再通讯，更好地利用带宽），也使得存储大小可控（每次通讯前积攒的存储大小是常量）。
	    - ZeRO-Offload/ZeRO-Infinity：核心思想是显存放不下的东西，放到其它地方。前者认为参数（fp16）、激活值计算量高，和前向反向传播相关；而参数（fp32）、优化器参数（fp32）和梯度（fp16）仅需要更新，计算量低，全部放入 CPU 也可。
- [张量并行(TP)与模型并行(MP)](https://zhuanlan.zhihu.com/p/622212228)【细节较多】
	- 基本算子：行维度拆分和列维度拆分。
	- MLP 层、Self-Attention 层、Embedding 层和 Cross-Entropy 层前后向传播流程与通讯量。
	- Megatron
		- [初始化](https://zhuanlan.zhihu.com/p/629121480)：定义模型的切割框架，并在此框架上初始化进程，分配 GPU 和设置进程组，未来每个进程独立执行自己所维护的那部分模型的计算。
			- 由于一般而言，通讯量 TP>DP>PP，因此通常优先让 TP 不跨机，同一台机器内带宽高。MP 设定原则则由 TP 和 PP 共同决定，主要需要预估峰值显存消耗。
			- 其中 `torch.distributed.init_process_group` API 较为重要，`init_method` 参数负责指明一个地址，进程组内的进程通过该地址中存放的信息进行交流（交流对象和内容），该信息只在进程 0 上存一份（避免冗余），或者也可以使用直接显式指明数据对象的 `Store` 参数，二者是互斥的。
			- 数据并行组 (DP) 的大小无需确定，确定了 TP 和 PP 后，对维护有相同模型部分的 GPU，就可以做数据并行，每个 DP 组的大小是 $World\_size/(TP_{size}*PP_{size})$，前者表示全局进程数。
			- 在 GPT 类模型中，输入层和输出层共享一个 `word_embedding`。因此，在计算完梯度，更新 embedding 权重前，输入和输出层需要进行通讯，保证 `word_embedding` 完全一致。也即 PP 组中的第一个和最后一个进程需要通讯，因此把它们也划分为一个组（分组的意义在于实现通讯）。
			- TP 组中，每次计算完有 AllReduce 过程聚合结果，然后才能进行下一层计算，此时输入将变为相同，可以使用 ZeRO-R 优化激活值避免冗余。
		- [模型切割](https://zhuanlan.zhihu.com/p/634377071)：在 CPU 上定义并初始化模型，然后将其搬运到当前进程所对应的 GPU 上。
			- 一般在 TP/PP 组内，设定不同的随机种子；而在 DP 组内，设定相同的随机种子。主要是要确认是初始化后切割（不同的随机种子），还是完成了 AllReduce 聚合操作此时 GPU 上的内容已经一致（相同的随机种子）。
			- Pytorch 默认是先在 CPU 定义出完整的模型，并对模型参数做初始化，然后根据进程 id 取出相应子模型，搬运到 GPU 上。如果自行搬运，还需要设定权重精度（例如将 fp32 降至 fp16）和初始化 DP 组（定义 DP 组间前向/后向传播、梯度计算和通讯等方法，可以使用 torch 的 DistributedDataParallel 类）。
			- MegatronModule：基类，同时令 PP 组的第一个进程和最后一个进程满足 `word_embedding` 完全一致。
			- 对 `word_embedding` 做拆分，`position_embedding` 和 `segment_embedding` 和输入相关，均不切割。
			- logit 实际表示的是 token 和词表中每个 token 的相似度，我们希望（token 和词表中所有词相似度的总和 -token 与真值间的相似度) /token 和词表中所有词相似度的总和这个值最小，这个差值就是最终的 loss，实际就是希望模型对真实值有较高的相似度，并且减少对其他词的高相似度。
		- [混合精度训练](https://zhuanlan.zhihu.com/p/662700424)
			- 总体流程：首先将参数复制一份，精度减半，fp32 的叫主权重（优化更新和最终结果均为它），fp16 叫训练权重。然后使用训练权重做前向计算（激活值也是 fp16），得到 fp32 的 loss（保证反向传播梯度计算可靠性）。接着将 loss 做 scale 处理，防止溢出（主要是梯度下溢）。使用反向传播计算梯度，以 fp16 的形式存储，但是在实际更新模型权重时，转换成 fp32（理论上此时 fp16 梯度已经无用）。最后还可以利用 Clip(由于阈值难定，一般是根据全量梯度的 L2 范数来裁剪，L2 范数也即先平方和再开方) 等操作来预防梯度爆炸/消失。
			- 由于舍入误差（基于梯度更新权重时因为超出范围导致没有变化，相当于空训了一轮）和梯度下溢（训练后期频繁出现梯度低于最小单位，同时利用 Scale 操作解决），所以仅利用 fp16 精度训练不妥。另外对于 BN/LN 等部分的权重，使用 fp16 可能造成训练不稳定，因此这部分参数从头到尾都使用 fp32。
			- Loss Scale：常量放大需要预先尝试不会发生梯度上溢，动态放大则先设定一个相当大的参数（因为目标是找到不会上溢的尽可能大的 scale 参数），然后出现上溢则缩小，连续若干次未遇到则放大。刚开始有上溢很正常，本来就需要经历一个探索过程。Megatron 两种方式都支持，动态情况下，连续指定次无上溢则放大，累计指定次上溢则缩小。任何 step 中任意一张卡上出现溢出均为直接作废，不使用本 step 计算的梯度更新权重。
			- 由于 Loss 经过了 Scale 操作，所以计算出的梯度也是 Scale 的结果，因此在更新权重前需要将梯度 UnScale。
			- 从 fp16 复制一份权重到 fp32，使用 `main_param = param.detach().clone().float()`，`detach()` 用于脱离计算图，也就是 `requires_grad = False`（反向传播不计算梯度），而 `clone()` 则避免 fp16 和 fp32 共享同一内存。
- [流水线并行(PP)](https://zhuanlan.zhihu.com/p/613196255)
    - 针对问题：朴素的模型并行存在 GPU 利用度不足，中间结果消耗内存大的问题。
    - 核心思想：在模型并行（通常更关注的是层内切成不同部分计算）的基础上，进一步引入数据并行的办法，即把原先的数据再划分成若干个 batch，送入 GPU 进行训练。
    - re-materialization（active checkpoint）：每个 GPU 上，只保留来自上一块最后一层计算结果的输入，相当于时间换空间。
    - 对于 BN，由于划分了 micro-batch，在训练时计算和运用的是 micro-batch 里的均值和方差，但同时持续追踪全部 mini-batch 的移动平均和方差，LN 则不受影响。
    - 这里的流水线并行是按层切开的，所以对于层数更深的模型，micro-batch 带来的显存节省效果更优。
- DeepSpeed MoE 并行训练

### Agent

> [!cite] 此时想到一段对话
> “北海，要多想。”
> “想了以后呢？”
> “我只能告诉你那以前要多想。”

- 通常是开放设计题，给个实际场景问设计一个相关的 Agent 要怎么考虑。

### AIGC Security

- 隐私
- 幻觉
- 越狱

*ps. 本文长期更新，欢迎常看淘金。*