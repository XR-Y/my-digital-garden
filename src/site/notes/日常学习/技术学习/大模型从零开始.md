---
{"dg-publish":true,"permalink":"/日常学习/技术学习/大模型从零开始/","tags":["Programming🖥️"],"noteIcon":"1","created":"2024-03-09T19:14:40.497+08:00","updated":"2024-09-09T13:30:24.455+08:00"}
---


<div id="summary">
本文介绍了大模型的基本结构和优化方法。在模型结构方面，讨论了注意力机制和 Encoder-Decoder 结构。在优化方面，涵盖了 BatchNorm 和 LayerNorm 的区别，以及常用的优化方法。此外，还介绍了 Bert 模型及其变体以及预训练和微调阶段注入知识的方法。另外，还探讨了 RAG 检索增强生成、LangChain 与向量数据库、思维链、强化学习 RLHF、分布式训练以及 Agent 等相关内容。
</div>

## 模型结构

> [!summary] 本质上这类问题是考基础，现有模型都是在标准的 Transformer 结构上修补。

### 基础概念

- 梯度消失：反向传播中，若某些层的梯度小于 1，随着层数的增加，梯度会指数级降低，最终靠近输入层时变得极小，导致权重更新缓慢，难以学习和优化。（e.g. Sigmoid 函数）
- 梯度爆炸：反向传播中，网络梯度值逐层放大，最终靠近输入层时变得极大，导致网络前层权重更新幅度大，难以稳定训练，甚至无法收敛。（e.g. ReLU 函数且权重较大时）

### [注意力机制](https://zhuanlan.zhihu.com/p/455399791)

- 为什么要有注意力机制
    - RNN 中 token 是一个个喂给模型的，随着序列长度的增加，模型下一步计算的等待时间越长，无法实现并行计算，而远距离间信息缺失情况也越明显。
    - 编码器中的自注意力机制：用于输入序列的全局信息交互和依赖关系建模，每个位置都可以与其它位置进行信息交换。
    - 解码器中的掩码自注意力机制：用于生成序列中的当前和之前位置的信息交互，防止生成时泄露未来的信息。
    - 解码器中的交叉注意力机制：结合编码器输出和解码器输入（Q 来自解码器当前层的输入，K 和 V 来自编码器输出），生成新的序列，使解码器能够利用编码器的全局上下文信息。
- 注意力机制的计算公式，一定要这样计算吗
- Transformer 为什么使用多头注意力机制，分析 [时间复杂度](https://zhuanlan.zhihu.com/p/514109726)
    - 捕捉多样化的特征，在不同子空间中学习不同的特征，多角度建模增强表达能力。
    - 增强模型的稳定性和泛化能力，避免单一的注意力头过拟合某些特定特征。
    - 并行计算，提高计算效率，尤其是在 GPU 环境下。
    - 提高上下文信息的捕捉能力，关注输入序列的不同部分。
    - 时间复杂度大致和单头的相似，均为 $O(n^2d)$，在不同子空间中并行地处理输入序列，从而捕捉更丰富的特征，提高模型的表现力和稳定性。
- 为什么 Q 和 K 使用不同权重矩阵生成，为什么不能使用同一个值进行自身点乘
    - 每个词的 query 和 key 具有不同的含义，它们需要不同的权重矩阵来捕捉不同的信息，可以让模型更加灵活地调整每个位置对其他位置的关注度，从而提高模型的表达能力。如果 Q 和 K 一样，乘积结果矩阵中，对角线的值会比较大，导致每个词会过分关注自身，从而降低模型的表达能力。
- 计算注意力为什么选择点乘而不是加法，从计算复杂度和效果上面讲区别
    - 二者复杂度都是 $O(d)$，前者可以利用矩阵乘法的硬件优化，后者需要额外经过 softmax 函数处理，增加了额外的复杂度。并且点乘可以自然度量相似性，加法捕捉向量间关系的能力较差。
- 为什么要对注意力进行 scaled（$\sqrt{d_k}$），公式推导
    - 之所以进行 scaling，是为了使得方差稳定，数据分布相对均匀，进而在 softmax 的过程中，梯度下降得更加稳定，避免因为梯度过小而造成模型参数更新的停滞。
- 在计算注意力时如何对 padding 做 mask 操作
    - pad 位置的注意力分数一般使用极大负数填充，后续 softmax 应用后这些位置的权重趋于零。
- 为什么在进行多头注意力时需要对每个 head 降维
    - 提高处理效率，避免过拟合（每个训练头都只表示一部分特征信息），最终输出时多头结果会被合并，仍旧保有原来的表达能力。
- transformer 是如何处理可变长数据的
    - RNN 是通过 timestep 的方式处理可变长数据，Transformer 是通过计算长度相关的 self-attention 得分矩阵来处理可变长数据。

### Encoder-Decoder

- 讲解一下 Encoder 的构造
- 为什么在获取词向量后需要对矩阵乘以 embedding size 的开方
- [位置编码](https://zhuanlan.zhihu.com/p/454482273)
    - Transformer 并行计算，每个 token 彼此独立，也就是输入是无序的，所以需要位置编码提供位置信息。
    - 位置编码的实现
        - 用整型标记：模型可能会遇到比训练序列更长的输入，不利于泛化，且长度越长编码值会很大。
        - 用 $[0, 1]$ 标记：当序列长度不同时，token 间的相对距离不一样。
        - 用二进制编码：编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的，无法使用位置向量来表示浮点型。
        - 需要有界又连续的函数：三角函数（sin）满足条件，通过频率控制 sin 函数的波长，越往右走（也就是越低位）波长越大，对变化越不敏感，也就是需要更高精度控制。但是仍然有一个问题，sin 是周期函数，因此从纵向（token 序列）来看，如果函数的频率偏大，引起波长偏短，则不同 t 下的位置向量可能出现重合的情况，所以在原论文中选用了一个非常小的值作为频率，尽可能拉长波长。
        - 使用 sin 已经实现了位置向量唯一、有界且可泛化（周期），现在还需要不同位置的位置向量可以通过线性变换得到。因此改为每个 token 位置向量中两两一组，sin 和 cos 交替出现，线性变换就由旋转实现， $\Delta t$ 时刻相当于旋转对应的角度。
    - 位置编码的性质
        - 两个位置编码的点积仅取决于偏移量 $\Delta t$ 。
        - 位置编码的点积具备无向性，即 $PE_t^T * PE_{t+\Delta t} = PE_t^T * PE_{t-\Delta t}$ 。
        - 综上两条，也就是位置编码的点积只能表示距离，无法表示方向。
- [残差结构及其意义](https://zhuanlan.zhihu.com/p/459065530)
    - $H(x)=F(x)+x$ ， $H(x)$ 表示恒等映射，残差主要用于解决网络加深后出现的退化问题。通过加深网络可以尝试让不同层学习不同特征，进而增强模型表达能力，但是由于退化和梯度消失，需要输出逼近输入，这样就能**保证深网络的效果不会比浅网络更差**，也就是 $F(x)$ 残差逼近 0。
    - transformer 中，对每一层的 attention 和 FFN，都采用了一次残差连接，也即每个位置当前层的输入和输出相加。
- [BatchNorm 和 LayerNorm 的区别，为什么选择用 LayerNorm](https://zhuanlan.zhihu.com/p/456863215)
    - 层间输入分布偏移问题（ICS）
        - 由于数据分布的变化，导致每层的参数依次受到前面的变动影响，在适应过程中训练的难度增大。
        - 在过激活层的时候，容易陷入激活层的梯度饱和区，降低模型收敛速度。比如 sigmoid 函数在绝对值很大时梯度就几乎消失。
        - 输入变动大，上层网络需要不断去适应下层变动，因此学习率不能设置过大，因为每一步确定性很低，但是这进一步降低了模型收敛速度。
        - 而无论是采用非饱和激活函数（比如 ReLU）、使用更小的学习率，或是更细致的参数初始化方法，以及数据白化（对每层输入做线性变化，如 PCA，调整方差均值，去除特征间关联），都是饮鸩止渴的方法，增大了模型运算量，也容易陷入超参数调整的复杂情境中。
    - BatchNorm
        - 对每一个 batch 进行操作，使得对于这一个 batch 中所有的输入数据，它们的每一个特征都是均值为 0，方差为 1 的分布。单纯限制分布在 $(0,1)$ 之间也不合理，降低了数据的表达能力，因此辅以线性变换，两个可学习超参数。
        - 在训练过程里，我们等一个 batch 的数据都装满之后，再把数据装入模型，做 BN。但是在测试过程中，我们却更希望模型能来一条数据就做一次预测，而不是等到装满一个 batch 再做预测。这有两个实现方法，一个是保留训练模型中，每一组 batch 的每一个特征在每一层的 $μ , \sigma$ ，这样显然消耗过高存储空间。另一个方法是 momentum，也即考虑过去的均值和当前的均值，比例通过超参数 $p$ 控制。
        - 除了解决 ICS 问题外，在规整数据分布后，也可以相应减轻极端值导致的过拟合问题，进而缓解对 dropout 的依赖和使用。
        - 后续的工作中，各个比较实验也证明了，ICS 问题并不一定导致模型表现差，同时 BN 对 ICS 问题的解决能力也是有限的，BN 有效的原因更可能是它使得优化曲线更加平滑。
    - LayerNorm
        - 背景：BN 在长度不一致的情况下存在问题，尤其是在文本问题中，文本中某些位置缺少足够的 batch_size 的数据，计算出的 $μ, \sigma$ 偏差明显。另外，由于测试中需要使用累积经验统计量，所以当测试文本长度超过训练文本时会缺少对应累积统计量。不过这只是理论上的，实际工程中都是多裁少 pad。也正是因为 LN 更适合处理长度不一致的情况，LN 成为了 NLP 问题中的默认 config，相当于有一个约定俗成的感觉。
        - 区别：相比 BN 在特征间进行标准化操作，LN 是在整条数据间进行标准化操作，也就是对于图像问题，BN 在某个特征通道上做各图标准化，LN 则是在一张图片所有通道和像素值范围内做标准化；对于文本问题，BN 是对整个小批量数据的所有序列 token 的所有特征维度标准化，LN 是对单个序列中单个 token 的所有特征维度标准化。
        - 也就是说，对于 LN，各条数据间在进行标准化的时候相互独立，每当来一条数据时，对这条数据的指定范围内单独计算所需统计量即可。
        - Transformer 论文中原始采用的是 Post-LN，而后提出的改进是使用 Pre-LN，二者的差异在 LN 和残差块（addition）的相对位置关系。Pre-LN 的优势是无需做 warmup，所以收敛速度更快，也规避了不必要的超参调整成本引入。具体原因是 Post-LN 在输出层的 gradient norm 较大（紧接在线性变换后，这里的线性变换是为了放大标准化 + 输出层梯度累积效应），往下层走则呈现下降趋势。因此这种情况下如果不使用 warmup 策略，在初期容易引起模型的震荡。然而也有观点表示 Pre-LN 会损失部分特征，因此同样条件下，Post-LN 能够借助更全面的特征信息得到更优结果。
- 前馈神经网络简单描述，使用了什么激活函数，公式，有何优劣
- Decoder 阶段的多头自注意力和 Encoder 有什么区别，为什么需要 sequence mask
- Transformer 的并行化如何体现，Decoder 端可以并行吗
- Encoder 是如何和 Decoder 交互的
- [teacher forcing、exposure bias 和 scheduled sampling](https://zhuanlan.zhihu.com/p/93030328)

### Bert

> GPT 采用 Masked-Attention，对模型和训练数据的要求会更高，因为模型能读到的信息只有上文。而采用普通 Attention 的 Bert 在训练阶段就能同时读到上下文。这个性质决定了 GPT 模型越来越大的趋势。
> 但是，长远来看，Masked-Attention 是 push 模型更好理解文字的重要手段，毕竟在现实中，我们更希望培养模型知上文补下文，而不是单纯地做完形填空。

- Bert 和原生 Transformer 的主要区别
    - 双向、同时在 MLM（遮罩语言模型）和 NSP（下一句预测，二元分类两个句子是否真实相连）两个任务上训练。
    - MLM 中，80% 使用 `<mask>`，10% 随机替换为词表中的其它词，10% 不变。主要是真实任务中不会有 `<mask>` 遮罩词，同时也适当引入噪声，增强泛化。
- 为什么会在开头加 `[CLS]`，可以有别的替代方案吗
- 为什么三个 embedding 可以相加（token、segment 和 position）
- [为什么要用 WordPiece/BPE 这样的 subword Token](https://zhuanlan.zhihu.com/p/460678461)
    - 基于空格的分词器：罕见词难表示，且容易出现大词表问题，token 的 one-hot 向量会很长，增加了 embedding 的训练参数量，并且冗余严重，同义不同形式的词算作不同词（loved，loving）。
    - 基于字符的分词器：单个字符本身缺乏语义，且增加了输入序列长度。
    - 基于子词的分词器：借助基本单元，相当于介于上述二者之间。实现是基于统计学的，不是基于语言学的。
        - Byte Pair Encoding（BPE）：统计连续字符对的出现频率，注意结尾要加上结束字符，相同的连续字符对出现在开头末尾的意义往往是不同的。
        - WordPiece：BPE 关注【频率最高】，WordPiece 关注【概率最大】。WordPiece 在每次合并时将选择一对相邻的子字符，这对子字符满足：当合并它们时，它们对语料库语言模型的概率提升最大。
        - Unigram Language Model (ULM)：BPE 和 WordPiece 没有关注相同单词的不同划分在不同场景下的表现。ULM 首先生成大词表，然后基于这张词表，对所有语料的所有子词划分结果考察，不断丢弃对总体评分贡献较低的子词。
- Bert 中是如何处理一词多义问题的
- Bert 为什么要使用 warmup 的学习率 trick
- 为什么说 GPT 是单向的而 Bert 是双向的，对比 NTP 和 MLM 任务
- prefix LM 和 causal LM 区别是什么？
    - Prefix LM（前缀语言模型）是一种生成模型，在生成时，前缀语言模型会根据给定的前缀（即部分文本序列，可以是上文，但不一定是上文）预测下一个可能的词。这种模型可以用于文本生成、机器翻译等任务，比如 GPT。
    - Causal LM（因果语言模型）是一种自回归模型，它只能根据之前的文本生成后续的文本，而不能根据后续的文本生成之前的文本。在训练时，因果语言模型的目标是预测下一个词的概率，给定之前的所有词作为上下文。这种模型可以用于多轮对话、语言建模等任务，比如 ChatGLM。
- Bert 变体 Roberta

### 常用优化

- Transformer 的两个缺点
    - 内存占用大：Transformer 的内存占用量随上下文长度而变化。这使得在没有大量硬件资源的情况下运行长上下文窗口或大量并行批处理变得具有挑战性，从而限制了广泛的实验和部署。
    - 随着上下文长度的增加，推理速度会变慢：Transformer 的注意力机制随序列长度呈二次方扩展，并且会降低吞吐量，因为每个 token 都依赖于它之前的整个序列，从而将长上下文用例置于高效生产的范围之外。
- GPT3 和 GPT2 的区别，发展史，InstrutGPT 解决对齐问题
    - [Codex](https://zhuanlan.zhihu.com/p/611313567)：GPT3 架构上，预训练 + 微调，微调数据选取算法网站和 Github 上使用 CI 的函数，包含单元测试，后续可用于评估代码是否准确。输出采用 Nucleus Sampling，取词至总和不小于 0.95，然后采样（随机性 + 确保不采到概率太低的词）。评价使用 $pass$@$k$ ，也即从模型的答案中随机抽取 k 个后，能从这 k 个里得到正确答案的概率。
- flash attention
- Multi Query Attention 和 Group Query Attention
    - MQA 和 GQA 都是节省 KV Cache 显存，前者是所有注意力头都共享同样的 K、V，后者是在原始的 MHA 和共享的 MQA 之间取权衡。
    - KV Cache：Attention 每层的计算仅需要当前 $Q_k$ 参与，而 K 和 V 全程参与计算，因此需要将每一步的 K 和 V 缓存起来。占用大小计算：$b(s+n)h*l*2*2=4blh(s+n)$，其中 $b$ 是批大小，$s+n$ 是输入输出长度，$h$ 是注意力头数，$l$ 是层数，两个 $2$ 分别是 KV 以及 Float16 占 2 个 byte。
- 位置编码
    - 长度外推问题
    - 旋转位置编码 RoPE
    - Attention with Linear Biases
- 分词 tokenizer
- Normalization
    - Layer Norm
    - RMS Norm
    - Deep Norm

## 大模型炼丹

> [!summary] 以下问题提纲挈领为主，考察更多是结合任务场景，大多无标准答案，主要靠积累

### 基础理解

- 下一个词序列预测：其实可以被看做是包含语法解析、语义理解和风格模仿的多任务学习。
- [过去三个月，LLaMA 系模型发展如何？指令微调的核心问题又是什么？](https://mp.weixin.qq.com/s/cXPNyOeK9vFjJcgxc_LqZQ)
- LLM 的训练目标
    - 最大似然目标，也就是最大化模型生成训练数据中观察到的文本序列的概率。具体来说，对于每个文本序列，模型根据前面的上下文生成下一个词的条件概率分布，并通过最大化生成的词序列的概率，使用梯度下降法等优化算法来优化模型参数。
- [涌现能力及其理解](https://zhuanlan.zhihu.com/p/621438653)
    - 两类任务： In Context Learning(Few Shot Prompt) 和思维链 (CoT)
- [为什么现在的大模型大部分是 Decoder only 结构](https://www.zhihu.com/question/588325646/answer/3357252612)
    - 泛化性能（zero shot、few shot）
    - 效率问题，复用 KV-Cache，多轮对话友好
        - [KV-Cache](https://zhuanlan.zhihu.com/p/686183300)：保存了 K 和 V 矩阵投影和每层向量的乘积结果，在计算注意力时空间换时间。成立的条件是每一个 token 的输出只依赖于它自己以及之前的输入，与之后的输入无关。Bert 就不符合这个条件，另外对位置编码有特别设计，每次增加新的 token 后会改变同一 token 位置编码的 LLM 也不符合。
    - 先发优势
    - 多样化解码生成
    - 讨论其它架构的模型：BERT、T5、BART、UNILM

### 常见问题

- 复读机问题
    - 数据偏差：数据集本身重复度高，多样性不足。
    - 训练目标：自监督学习可能使得模型更倾向于生成与输入相似的文本。
    - 解码方式：贪婪搜索中 beam 和采样中温度的设置。
    - [Self-Reinforce](https://zhihu.com/question/616130636/answer/3166309896)
- 长文本处理
    - 分块或分层（段落、句子等）处理，可适当引入重复以确保连贯性。
- 领域微调后，通用能力遗忘问题
    - 数据方面：保留一定比例的通用数据。
    - 训练方面：领域自适应、对抗学习、强化学习、多任务学习和增量学习（交替使用通用数据集和领域数据集，或逐步引入）。
- 不同任务场景下的数据集格式
- [幻觉问题](https://www.zhihu.com/question/635776684)
    - 什么时候容易产生幻觉
        - 数值混淆，尤其是涉及数字和日期时。
        - 长文本处理，长期依赖关系中可能存在自相矛盾内容。
        - 逻辑推断障碍，模型错误解读了源文本中信息。
        - 上下文与内置知识的冲突，比如领域知识和预训练知识中相同词语的不同意义。
        - 错误的上下文知识，尤其是上下文包含错误假设时，模型可能难以识别。
    - 解决：从数据、训练和推理三个角度讨论。
    - [模型遗忘](https://zhuanlan.zhihu.com/p/665691828)：用于版权、隐私保护和偏见信息处理。模型的遗忘不是完全对目标信息返回为空，而是出现幻觉，比如哈利波特的角色信息被遗忘，返回他是一个导演作家等。
        - 实际策略：先对要遗忘的信息构建特定数据集，使其对该文本的预测更加倾向于原始内容。也就是改变最开始模型的相关标记，从而方便下一步，使用模型自身的预测能力，为每个标记生成替代标签。
- 节省显存
    - 存储分类
        - 模型必要相关的：模型梯度、模型参数和优化器状态。
        - 非模型必须的：激活值（反向传播中计算梯度更快）、临时存储（例如把梯度发送到某 GPU 总聚合）、碎片化存储空间。
    - 梯度累积 Gradient Accumulation
    - 网络剪枝
    - 量化方法：FP32、FP16 和 BF16。
        - fp16 中 5 位用来表示指数位（除去全 0 和全 1，一共是 $2^5-2=30$，也就是可以表示 $[-14,+15]$），10 位用来表示小数位，剩下 1 位是符号位。其中指数位全 0 表示非规格数（+0、-0 和极其接近 0 的数字），全 1 表示特殊数（小数位全 0 表示 Inf，符号位确定 +Inf 和 -Inf，小数位不全为 0 表示 NaN），所以 fp16 的最大绝对值是 $(1+\frac{1023}{1024})*2^{15}=65504$，而最小正值是 $\frac{1}{1024} *2^{-14}=2^{-24}=5.96E−8$。
        - fp32 和 fp16 基本一致，符号位 1 位，指数位 8 位，尾数位 23 位，因此 fp32 的动态范围为 $(1.4E-45$ ~ $3.40E38)$。
        - bf16 则是保留了和 fp32 一样的指数位，也就是在尾数位做了截断，只剩下 7 位尾数位，因此就是牺牲了精度来换取几乎和 fp32 一样大的取值范围，避免 fp16 的溢出问题。bf16 的最大绝对值是 $(1+\frac{127}{128})*2^{127}$，而最小正值是 $\frac{1}{128} *2^{-126}=2^{-133}$。bf16 的动态范围为 $(9.2E-41$ ~ $3.38E38)$。
        - 为什么要使用半精度
            - 占用内存更少，可以选用更大的 batch_size。
            - 训练时通信量大幅降低（尤其是多机多卡），加快数据流通，大幅降低等待时间。
        - 混合精度训练
            - 不能全部替换为半精度：**溢出问题**（超过正负最大值）和**舍入误差**（比最小正值小的加减被舍去）
            - 权重的高精度备份：使用 fp32 权重作为精确的 “主权重 (master weight)”进行备份，而其他所有值（weights，activations， gradients）均使用 fp16 进行计算以提高训练速度，最后在梯度更新阶段再使用半精度的梯度更新单精度的主权重，这样当很小的梯度乘上学习率后要跟权重（fp32 的）做运算时，就不会被舍弃了。
            - Loss Scale：大部分的梯度值都很小，因此该方法通过让梯度乘一个 scale，从而整个分布右移、占据更多 fp16 可表示的范围。
            - 算数精度：神经网络主要涉及三种计算，向量点乘（常见于全连接层），归约（Reduction，减少张量元素，常见于归一化和池化层等），点运算（pointwise，常见于 ReLU、tanh 等激活函数），其中向量点乘中加法使用 fp32 完成，存储结果则使用半精度，reduction 也要用 fp32 来做，但以半精度方式存储；而 pointwise 的运算主要受到 memory-bandwidth 限制（计算简单且可以良好并行化），因此它们是以单精度还是半精度运算，都不影响计算速度，所以单、半精度均可。
        - 由于 fp16 的范围有限，因此在训练中很容易出现溢出问题，因此一般**在训练时都使用 bf16**，若要使用 fp16 则通常搭配 loss scale 操作。而 bf16 能够和 fp32 有相近的范围，且由于表示范围略小于 fp32，可以起到**隐式正则化**的作用，避免过拟合（训练时参数通常呈现幂律分布，在 bf16 中,outlier 值由于超出了表示范围而被 clipped，且大部分值仍在范围内，不会明显降低模型性能）。而在推理时，由于 fp16 精度更高，参数范围也通常趋于稳定，所以通常都直接使用 fp16。
        - 由于额外增加的反量化和重量化操作的存在，推理和训练一般都不会节省时间，反而会增加时间，因此也勉强可以算是一个时间换空间的例子。
        - 半精度仍然不够的情况下，也可以进行 8bit 或 4bit 量化。
        - TF32：由 1 个符号位，8 位指数位（对齐 FP32）和 10 位小数位（对齐 FP16）组成，实际只有 19 位。在性能、范围和精度上实现了平衡，用于替代 FP32。
        - NF4：将浮点权重在量化的同时归一化到以 0 为均值、标准差在 $[-1,1]$ 范围内的正态分布上。首先将浮点权重参数离散化为 4 位整数值，然后计算正态分布固定期望值，最后标准化，于 QLoRA 首次使用。
    - 知识蒸馏：大型预训练模型上进行推理，并使用其输出作为目标标签，来训练一个较小的模型。

### 微调相关

- 预训练和微调是哪个阶段注入知识的
    - 知识注入是在预训练阶段进行的，预训练模型通过大规模通用数据的训练，学习到了丰富的语言知识和表示能力。
    - 微调的目标是将预训练模型中学到的通用知识和能力迁移到特定任务上，提升模型在目标任务上的性能。
- 微调是啥
    - 冻结底层权重（通常卷积层） + 替换顶层分类器（通常全连接层） + 解冻部分权重（可选）
- [提示微调](https://zhuanlan.zhihu.com/p/635686756)
    - BitFit
    - Prefix Tuning
    - Prompt Tuning
    - [P-Tuning](https://zhuanlan.zhihu.com/p/635848732)
    - P-Tuning v2
- 适配器微调
    - [Adapter Tuning](https://zhuanlan.zhihu.com/p/636038478)
    - AdapterFusion
    - AdapterDrop
    - [MAM Adapter](https://zhuanlan.zhihu.com/p/636362246)
    - UniPELT
- [LoRA](https://zhuanlan.zhihu.com/p/636215898)
    - 主要思想：冻结预训练模型的参数，并选择用 A 和 B 矩阵来代替，在微调下游任务的时候，只更新 A 和 B。
    - 优势：节省资源、共享模块（替换 AB 矩阵快速切换下游任务，即**可插拔**）、不会引入推理延迟（全量微调后需要重新读写内存、缓存失效、针对参数优化失效）、和其它许多方法正交（如 Prefix Tuning）。
    - 基础模型对 LoRA 训练影响较大，增大数据量和参数量可以提升效果，但是并不是仅通过提升数据量和参数量就可以让本来学习力更低的模型效果超越学习力更强的模型。
    - [LoRA](https://zhuanlan.zhihu.com/p/646831196)
        - 对 A 矩阵随机高斯初始化，对 B 矩阵零初始化，保证**训练开始时旁路为 0 矩阵**，不会在开始引入噪声，理论上二者初始化情况也可以交换，并没有明显差异。
        - 另外有缩放因子 $\frac{\alpha}{r}$，其中秩 $r$ 就是信息量的表现形式。已知 SVD（奇异值分解）能够关注到最强调的几项特征，而 $W$ 代表旧知识，对其作 SVD 没有意义，$\Delta W$ 代表新知识，但是该项并不是确定的，只有全参微调后才能确定。因此最终只能设置**秩为超参**，让模型自行学习低秩矩阵 A 和 B。除此之外， $r=64$ 的前八个特征并不一定和 $r=8$ 完全相同，模型会尽可能往信息最丰富的维度学，但不一定学出来的就是真实的 top $r$ 。因此我们在模型的不同部分，比如 $W_q$ 和 $W_v$ ，也可能采用不同的秩。
        - $\alpha$ 则一般设置为第一次实验时的 $r$ ，第一次的 $r$ 通常会设置较大，从而尽可能更近似 $\Delta W$，此时缩放因子为 1，意味着假定 LoRA 微调效果和全参微调持平。而后自然会逐渐减小 $r$ ，此时缩放因子随之增大，也就是保持新知识对模型权重的影响。除此之外， $r$ 较小意味着精炼但不全面，梯度下降方向更加确定，可以适当放大影响；而 $r$ 较大意味着全面而有冗余或噪声，适当减小步伐也是合理的。
        - 正常情况下，输入的信息因为注意力机制会关注重要信息和省略无意义信息而导致信息冗余，体现在矩阵上就是**不满秩**，因此使用这种方式可以大幅降低参数计算量。同时由于噪声的影响，可能在某些场景下效果甚至能超过全量微调。
    - [AdaLoRA](https://zhuanlan.zhihu.com/p/657130029)
        - 主要思想：LoRA 中对不同模块使用相同的秩，且秩设置不变均不合理，所以总体目标就是**动态调整不同模块下的秩**。微调高层参数（比如 FFN）的效果会比微调底层参数（比如 attention）的效果更好。
        - LoRA 使用了 $\Delta W=U\Sigma V^T\approx BA$ 近似，而 AdaLoRA 则直接使用三个矩阵 $P, \Lambda, Q$ 去近似，其中中间 $\Lambda$ 矩阵是对角矩阵，初始化为 0，另外二者随机高斯初始化，原因同样是保证训练开始时无噪声。
        - 实际计算中， $\{P_{*,i}, \sigma_{i}, Q_{i,*}\}$ 组成三元组，$P$ 表示第 $i$ 列，$Q$ 表示第 $i$ 行，根据重要性分数，将不重要的三元组中的 $\sigma$ 置为 0，相当于 mask，**不是直接删除**的原因是，模型学习是探索的过程，在一开始模型认为不重要的三元组，在后续过程中模型可能会慢慢学到它的重要性。能够保留奇异向量，也是 AdaLoRA 表现优于 LoRA 的一个原因。
        - 三元组的重要性分数 = $\Lambda$ 的重要性分数 + $P$ 矩阵中所有元素重要性分数的均值 + $Q$ 矩阵中所有元素重要性分数的均值。取均值的原因，是不希望参数量影响到重要性分数。而单参数的重要性分数则定义为参数权重和损失函数在该参数上的梯度乘积的绝对值，然后加以 momentum 消除不同 batch 上的波动，也就是减轻单个 batch 样本带来的重要性的评估误差，对于这个引入的不确定性 $U$ ，在计算时也考虑平滑后的差异，不可忽略真实的波动情况。
        - 筛选重要三元组的策略称为 top_b，在训练刚开始的过程中逐渐增大 top_b，也就是加秩，让模型充分探索，到后期开始逐渐降低 top_b，最后以相对稳定的 top_b 进行训练，整个过程和 warmup 类似。
        - AdaLoRA 的损失函数由两部分组成，一部分是正常的训练集损失函数，即预测值和真实标签之间的差距，而另一部分是 $P$ 和 $Q$ 和**满足正交矩阵性质**的差异，因为真实的 SVD 中，$U$ 和 $V$ 都是正交矩阵。由于 LoRA 在训练时没有引入任何和 SVD 性质相关的约束，所以往往 AdaLoRA 比之能够具有更好的效果，训练时能够更加稳定，泛化性能更好。
    - [QLoRA](https://zhuanlan.zhihu.com/p/666234324)
        - NF4：四位标准浮点数量化，其中结合了**分位数量化**（分为若干相等块，使用累积分布函数的反函数简化计算）和**分块量化**（将张量分成若干个块，每个块都有独立的量化常数，也就是该块最大值，对其进行归一化，反量化就借助其和存储的量化后的低精度值恢复到高精度值，分块的优势在于不容易出现极端值，导致整体量化后极大或极小）。关于 0 的处理是正数取 9 个值，负数取 8 个值，均会取到零点，然后去重，也就是确保 0 的映射值是 0，并用满 4 位数据类型的全部 16 位。
        - 双重量化：由上得知，在模型保存时，除了保存量化后的低精度值，也要保存对应的量化常数。而这个常数是 FP32 高精度值，会额外占用较高显存，因此需要对这个常数也做一次 8bit 量化，QLoRA 以每 256 个量化常数为一组。同样，在反量化时，也因此需要两次操作。
        - 分页优化：当显存不足时，将保存的部分梯度检查点转移到 CPU 内存上，和计算机的内存数据转移到硬盘上的常规内存分页类似，牺牲时间换空间，主要是解决显存**峰值占用**问题，论文的创举是在消费级显卡上训练 33B 模型，在此场景下应该是必需的。是对梯度检查点的进一步优化，梯度检查点是在牺牲显存保存前向传播的激活值，和节省显存重新依照损失函数计算激活值之间的权衡。丢失部分激活值，有保存的就用，没有就重新计算。
        - QLoRA 也在原参数一侧添加了一个与原参数并行的低秩适配器，它的精度是 BF16。也就是说，QLoRA 有一个 4NF 的存储数据类型和 16BF 的计算数据类型，在计算前向和反向传播时通过反量化为 BF16 计算。
        - QLoRA 主要关注点都在尽可能节省显存，而在微调训练方面几乎和 LoRA 一致。

### RAG 检索增强生成

- 解决问题
    - 长尾知识：相对通用和大众的知识结果更准确，剩下的通过增大训练集或者增加参数性价比较低，通过检索知识在上下文给出更加经济。
    - 数据新鲜度：无需重新训练模型就加入更新的知识，因此频繁更新的知识建议单独作为外部知识库。
    - 私有数据：在训练中加入私有数据成本较高，且有隐私信息泄露风险。
    - 来源验证和可解释性：在生成的结果和信息来源之间建立关联，约束生成空间，注意力更加关注知识库中内容，可以缓解幻觉问题。
- RAG vs. SFT
    - 数据方面，RAG 能够确保数据保持最新，在面对频繁变更的数据时成本消耗也偏低。
    - 生成方面，RAG 不容易产生幻觉，但是无法保证形成特定风格的输出，响应阶段相对透明（比如可以提供检索的匹配度）。
    - 实现方面，RAG 关注的核心是检索策略以及数据的存取和更新，SFT 则是数据集的构建、微调目标定义和计算资源准备。
- 索引形式
    - 链式索引
    - 树索引
    - 关键词表索引
    - 向量索引
- 查询变换
    - 同义改写/扩展查询：生成多个类似的查询，然后各个问题都去找文档，需要去重，并且可能分散注意力，因此这种方法必须要搭配重排序。
    - 查询分解：将一个查询分解为多个子问题。
    - HyDE：假设文档回复，也就是先生成一个答案，根据这个问答去查询，可能产生幻觉。
- 检索和重排序：初检索注重效率，选择出 TOPK 召回，然后由重排序（比如时间/时效性）进行精确比对。需要重排序：将一个文档变为向量后势必损失一些信息。另外，重排序后也可以保留尽量少但是高相关的文本，减小上下文长度提升性能。
	- Bi Encoder：每个输入文本独立编码，嵌入向量可以预先计算和缓存，适合高效匹配大规模语料库场景，因此被用于 Retriever。
	- Cross Encoder：将两个文本拼接作为联合输入，通常是以 `[CLS]` Text1 `[SEP]` Text2 的形式输入到一个编码器中，模型可以捕捉到它们之间的精确交互和复杂关系，但是效率低下（每次都需要重新计算无法预存），因此被用于 Reranker。
- Retriever 选择
    - Sparse Retriever：如 BM25，**效率高**尤其是针对大规模文本库；**可解释性强**，词频词匹配等规则帮助用户理解为什么能够检索成功；**部署简单**，所需的计算资源相对较少。
    - Dense Retriever：**语义理解能力强**，能够捕捉查询和文档之间的深层语义关系；可以**处理复杂查询**，尤其是需要上下文理解或多层次语义查询的场景。
    - 中庸的策略是使用 Sparse Retriever 做初步检索，再使用 Dense Retriever 做精排，本质上和搜推的思路类似。
    - 在训练 Dense Retriever 时，使用**对比学习**，构建正负样本对非常必要，尤其是负样本，需要关注**难负样本的质量**，否则模型在检索到无关文本时很可能效果骤降。
    - Learned Sparse Retriever(e.g. BGE-M3)：先通过 BERT 等深度学习模型生成 dense embedding，再引入额外的步骤对以上 dense embedding 进行稀疏化，得到一个 sparse embedding。
	    - BERT 在相似比较时仅关注第一个 `[CLS]`token，而 BGE-M3 扩大到关注序列中所有 token，dense retrieval 仍然使用 `[CLS]`。
	    - 在 encoder 输出层上又增加一个线性层和 ReLU 激活函数，得到每个 token 的权重（有重复出现的 token 取 max），此时就可以使用和 TF-IDF 相似的思想，开展 sparse retrieval，计算两组向量相似度使用的是共现计算。同时 ReLU 的结果是非负的，有助于 embedding 的稀疏性。
	    - 一阶段使用 RetroMAE 的对比学习，训练 dense 向量。将低掩码率的的文本输入到 encoder 中得到 embedding 向量，将该 embedding 向量与高掩码率的文本输入到浅层的 decoder 向量中，输出完整文本。这种预训练方式迫使 encoder 生成表征能力强的 embedding 向量，在表征模型中提升效果显著。
	    - 二阶段使用自蒸馏学习（教师模型和学生模型相同），奖励由 dense score、sparse score 和 multi-vector score 三部分组成（1:0.3:1），也就是说模型是参考 dense 和 multi-vector，优化 sparse 的。
- 调优痛点
    - chunk_size（块大小）和 similarity_top_k 的超参数选择。
    - 重排序，一方面解决相关性问题（确保找到相关的文档，而不是只包含关键词的文档），另一方面缓解中间丢失问题（模型注意力基本集中于开头和结尾的信息）。
    - 检索策略选择
        - Basic retrieval from each index：基础检索，向量/关键词/结构化字段值。
        - Advanced retrieval and search：高级检索，查询权重/句子级别/最大边际相关（考虑结果多样性和查询相关性）。
        - Auto-Retrieval：自动检索，自动选择合适的策略和参数。
        - Knowledge Graph Retrievers：知识图谱检索，适用于百科等场景。
        - Composed/Hierarchical Retrievers：复合/层次检索，比如 BM25-SVM 复合检索器。
    - 格式解析，尤其是在需要特定格式输出的场景。
    - LLamaIndex 支持并行化处理，尤其是在系统处理大量数据的场景。
    - 结构化数据和 PDF 等文件处理，采用符号推理和文本推理结合（多数投票机制）。另外，也可以尝试使用 pdf2htmllex 将 PDF 转换为 HTML
    - 备用模型
    - 安全性，对抗提示注入，处理不安全的输出，防止敏感信息的泄露，相关工具有 LLama Guard。
- Graph RAG
- 多模态 RAG

### LangChain 与向量数据库

### 思维链

### 强化学习 RLHF

- 基本思想：智能体观察到环境状态 $S_t$ 和奖励 $R_t$，输出动作 $A_t$，$A_t$ 进一步触发环境的变化 $S_{t+1}$ 和新的奖励 $R_{t+1}$。目标是确定一个策略，能够根据当前观测到的环境状态和奖励反馈，来选择最佳的动作。
- 和 NLP 的契合点：token 是一个个被输出的，因此正好对应了输出动作，而产出的新 token 对应即时奖励，上文 → 上文 + 新 token 对应了状态的转换。实际并不是每个 token 都去更新一次参数，而是有足够的观测数据后。
- 四个基本模型
    - Actor Model：演员模型，对应想要训练的目标语言模型，需要训练。
        - Loss 计算：$actor\_loss=-Adv_tlogP(A_t|S_t)$，其中 $Adv_t$ 表示优势，优势大于 0 那么减小 loss 就需要增大概率，反之优势小于 0 就需要减小状态 $S_t$ 下执行 $A_t$ 的概率。而优势定义如下：$Adv_t=(R_t+\gamma*V_{t+1}-V_t)+\gamma* \lambda *Adv_{t+1}$。由于最后一个时刻优势为 0，那么从后向前计算即可。
        - 为了重复利用 1 个 batch 的数据来做 $ppo\_epochs$ 次模型更新，实际使用 $\frac{P(A_t|S_t)}{P*{old}(A_t|S_t)}$ 用于计算。
    - Critic Model：评论家模型，预估总收益（包含即时和未来），由于未来收益不可知仅能预测，因此也需要训练，最简单的初始化方式是采用 Reward Model。
    - Reward Model：奖励模型，计算即时奖励，参数冻结。
    - Reference Model：参考模型，基于 KL 散度防止模型效果相对 SFT 差异过大，参数冻结。

### 多模态大模型

- CLIP(Contrastive Language-Image Pre-Training)
    - 搜集了大量图像 - 文本对，分别使用 resnet/vit 编码图像、transformer 编码文本，两两计算余弦相似度，只有对角线上的元素对是正样本，其余都是负样本。
    - ViT(Vision Transformer)：将图像拆分为固定大小的 patch，然后对每个 patch 展平为一维向量（通常使用的隐藏层维度是 768），接着加入位置编码（指示每个 patch 在原始图像的位置），最后将包含位置信息的 pathc 向量输入 transformer，结尾一般添加有 `CLS` 分类 token。
    - 对比学习的训练目标（loss）就是最大化正样本的相似度预测，同时最小化负样本的相似度，ITC 损失函数由图像和文本两部分平均，分别是当前图像 - 文本对的余弦相似度相对于总余弦相似度的差距。也就是说是双向的，既要优化从图像预测文本，也要预测从文本预测图像。
    - 实际文本使用“这是一张 `label` 的图片”，能够识别从未在训练数据中出现过的类别，只需要将候选 `label` 中加入可能的未出现过类别。
- ViLT(Vision-and-Language Transformer)
    - 相比起 CLIP 更注重交互而不是嵌入，图像使用 ViT 同时文本词嵌入后都加入模态类型标志，合并成统一序列输入 transformer，使用 preNorm，训练更稳定但可能丢失特征信息。
    - 训练目标是 Image Text Matching（预测图像和文本是否匹配）和 Masked Language Modeling（完形填空任务）。
    - 轻量级、统一使用 transformer 来处理视觉和文本特征，大幅提高了效率，推理很快但是训练较慢。
- ALBEF(Align Before Fuse)
    - 动量蒸馏（MoD)：解决即使是正样本对也存在不相干文本 token 或图像中其它物体的问题，本质上是提高对噪声的鲁棒性。
- BLIP(Bootstrapping Language-Image Pretraining)
    - 由两个单模态编码器（Image Encoder, Text Encoder）、一个以图像为基础的编码器（Image-grounded Text Encoder）和一个以图像为基础的解码器（Image-grounded Text Decoder）组成。
    - 损失函数：ITC、ITM 和 LM。
    - CapFilt（Caption Filtering）机制：处理噪声。
        - Captioner 是 image-grounded text decoder，它在人工标注数据集上以 LM 为目标进行微调，对给定的图像进行文本解码。
        - Filter 是 image-grounded text encoder，它根据 ITC 和 ITM 的目标进行微调，以学习文本是否与图像匹配，去除原始网络文本和合成文本中的噪音文本。
        - Bootstrap 过程，Captioner 生成的图文对与 Filter 过滤后的网络图文，再加上人工标注的图文对结合起来，形成一个新的数据集，重新预训练一个新模型。
    - BLIP2：引入了 Q-Former，负责弥合视觉和语言两种模态的差距，由 Image Transformer 和 Text Transformer 两个子模块构成，它们共享相同自注意力层。
        - 使用 ITC、ITM 和 ITG 预训练对齐，然后冻结的 Image Encoder 生成原始的图像特征，而 query tokens 和 Q-Former 从原始图像特征中生成转化好的图像特征，然后该图像特征经过全连接层映射到 LLMs 的文本 embedding 空间中。然后这些映射后的图像特征，就相当于视觉 prompts，和文本 embedding 一起，输入到冻结的 LLMs 中，最后生成目标文本。
        - Q-Former 作为视觉语义提取器，本身非常难学好。

### 分布式并行

- 优化目标：更快地训练更大的模型，难点在于 GPU 的内存限制（更多的参数量、中间结果和训练数据，对应更大模型）和 GPU 间的带宽限制（通信时间，对应更快训练）。
- [数据并行(DP)](https://zhuanlan.zhihu.com/p/617133971)
    - DP：计算 GPU 称为 Worker，梯度聚合 GPU 称为 Server。在实际应用中，为了尽量减少通讯量，一般可选择一个 Worker 同时作为 Server。
        - 存在的问题有两个，一个是每个 GPU 都存了一份模型，冗余严重；另一个是通讯开销大，Server 一般成为瓶颈，针对这个问题可以采用异步的思想解决，但对一个 Worker 来说，只是等于 W 不变，batch 的数量增加了而已，在 SGD 下，会减慢模型的整体收敛速度。
        - 受限于通信负载不均，DP 一般适用于单机多卡场景。
    - DDP：分布式数据并行，首先需要解决通信负载不均的问题，思路是将 Server 的通信负载平摊到每一个 Worker 上，称为 Ring-AllReduce。
        - Reduce-Scatter：每个 GPU 只和相邻两块 GPU 通信，定义一个环状数据通信路线，直至每一块 GPU 上都有一块数据拥有了对应位置完整的聚合。
        - All-Gather：目标是把红色块的数据广播到其余 GPU 对应的位置上，相当于上一步是在数据合并，这一步是直接替换。
        - DP 和 DDP 的总通讯量相同，只是分摊问题导致 DP 需要更多的时间搬运数据。
    - [ZeRO]()：本质上是将能切分的都切分，用较小增加的通讯量换回大量存储量的节省。
	    - ZeRO-DP（针对模型必要或相关存储内容的优化）：模型并行的形式，数据并行的实质。模型并行是针对同样的输入，只使用自己维护的那部分参数来计算结果后整合，而 ZeRO 是在整合权重后，根据不同的输入计算结果后聚合。
	    - ZeRO-R（针对其余存储内容的优化，需要更灵活的手段）：针对激活值，同样采用切分并且可以灵活选用保存和重新计算的比例。针对碎片空间，设置机制适时整合存储空间。针对临时存储，设置固定大小的内存 Buffer，可以提高带宽利用率（GPU 数量上升切片会减小，可以积攒数据再通讯，更好地利用带宽），也使得存储大小可控（每次通讯前积攒的存储大小是常量）。
	    - ZeRO-Offload/ZeRO-Infinity：核心思想是显存放不下的东西，放到其它地方。前者认为参数（fp16）、激活值计算量高，和前向反向传播相关；而参数（fp32）、优化器参数（fp32）和梯度（fp16）仅需要更新，计算量低，全部放入 CPU 也可。
- [张量并行(TP)与模型并行(MP)](https://zhuanlan.zhihu.com/p/622212228)【细节较多】
	- 基本算子：行维度拆分和列维度拆分。
	- MLP 层、Self-Attention 层、Embedding 层和 Cross-Entropy 层前后向传播流程与通讯量。
	- Megatron
		- [初始化](https://zhuanlan.zhihu.com/p/629121480)：定义模型的切割框架，并在此框架上初始化进程，分配 GPU 和设置进程组，未来每个进程独立执行自己所维护的那部分模型的计算。
			- 由于一般而言，通讯量 TP>DP>PP，因此通常优先让 TP 不跨机，同一台机器内带宽高。MP 设定原则则由 TP 和 PP 共同决定，主要需要预估峰值显存消耗。
			- 其中 `torch.distributed.init_process_group` API 较为重要，`init_method` 参数负责指明一个地址，进程组内的进程通过该地址中存放的信息进行交流（交流对象和内容），该信息只在进程 0 上存一份（避免冗余），或者也可以使用直接显式指明数据对象的 `Store` 参数，二者是互斥的。
			- 数据并行组 (DP) 的大小无需确定，确定了 TP 和 PP 后，对维护有相同模型部分的 GPU，就可以做数据并行，每个 DP 组的大小是 $World\_size/(TP_{size}*PP_{size})$，前者表示全局进程数。
			- 在 GPT 类模型中，输入层和输出层共享一个 `word_embedding`。因此，在计算完梯度，更新 embedding 权重前，输入和输出层需要进行通讯，保证 `word_embedding` 完全一致。也即 PP 组中的第一个和最后一个进程需要通讯，因此把它们也划分为一个组（分组的意义在于实现通讯）。
			- TP 组中，每次计算完有 AllReduce 过程聚合结果，然后才能进行下一层计算，此时输入将变为相同，可以使用 ZeRO-R 优化激活值避免冗余。
		- [模型切割](https://zhuanlan.zhihu.com/p/634377071)：在 CPU 上定义并初始化模型，然后将其搬运到当前进程所对应的 GPU 上。
			- 一般在 TP/PP 组内，设定不同的随机种子；而在 DP 组内，设定相同的随机种子。主要是要确认是初始化后切割（不同的随机种子），还是完成了 AllReduce 聚合操作此时 GPU 上的内容已经一致（相同的随机种子）。
			- Pytorch 默认是先在 CPU 定义出完整的模型，并对模型参数做初始化，然后根据进程 id 取出相应子模型，搬运到 GPU 上。如果自行搬运，还需要设定权重精度（例如将 fp32 降至 fp16）和初始化 DP 组（定义 DP 组间前向/后向传播、梯度计算和通讯等方法，可以使用 torch 的 DistributedDataParallel 类）。
			- MegatronModule：基类，同时令 PP 组的第一个进程和最后一个进程满足 `word_embedding` 完全一致。
			- 对 `word_embedding` 做拆分，`position_embedding` 和 `segment_embedding` 和输入相关，均不切割。
			- logit 实际表示的是 token 和词表中每个 token 的相似度，我们希望（token 和词表中所有词相似度的总和 -token 与真值间的相似度) /token 和词表中所有词相似度的总和这个值最小，这个差值就是最终的 loss，实际就是希望模型对真实值有较高的相似度，并且减少对其他词的高相似度。
- [流水线并行(PP)](https://zhuanlan.zhihu.com/p/613196255)
    - 针对问题：朴素的模型并行存在 GPU 利用度不足，中间结果消耗内存大的问题。
    - 核心思想：在模型并行（通常更关注的是层内切成不同部分计算）的基础上，进一步引入数据并行的办法，即把原先的数据再划分成若干个 batch，送入 GPU 进行训练。
    - re-materialization（active checkpoint）：每个 GPU 上，只保留来自上一块最后一层计算结果的输入，相当于时间换空间。
    - 对于 BN，由于划分了 micro-batch，在训练时计算和运用的是 micro-batch 里的均值和方差，但同时持续追踪全部 mini-batch 的移动平均和方差，LN 则不受影响。
    - 这里的流水线并行是按层切开的，所以对于层数更深的模型，micro-batch 带来的显存节省效果更优。
- DeepSpeed

### Agent

### AIGC Security