---
{"dg-publish":true,"permalink":"/日常学习/技术学习/离线强化学习/","noteIcon":"1","created":"2023-10-21T09:43:45.623+08:00","updated":"2023-10-21T11:14:00.759+08:00"}
---

# 起源
传统强化学习是在线（online）过程，通过智能体和环境不停交互获取数据来评估和改进策略。但往往实时交互成本较高，还存在安全风险（如自动驾驶等），并且需要消耗较长的训练改进策略时间。
因此，离线强化学习就是利用已有数据集（历史数据集），尽可能达到和传统在线强化学习的效果。

# 核心问题
分布偏移，即数据集不能覆盖所有的(s,a)情况，而Q函数对这种数据集外情况的估计往往是不精确的，因此可能选择到实际收益很低的动作，导致整体效果很差。
离线强化学习本质上也是权衡，要基于数据集学习一个优于行为策略的策略，同时也要最小化和行为策略的误差，避免分布偏移。

# 数据集
## 开源基准数据集
[D4RL: A collection of reference environments for offline reinforcement learning](https://github.com/Farama-Foundation/D4RL)

## 相关研究
### [Understanding the Effects of Dataset Characteristics on Offline Reinforcement Learning](https://openreview.net/forum?id=A4EWtf-TO3Y)
五种数据集生成策略：
- 随机数据集：使用基于随机动作固定的策略生成，作为基线。
- 专家数据集：训练一个在线策略直到收敛，并用这个最终的专家策略生成所有样本，无探索。
- 混合数据集：20%随机数据集，80%专家数据集。
- 噪声数据集：专家策略训练中引入了ɛ贪婪，贪婪系数为0.2，使用这个训练过程带噪声的专家策略生成数据。
- 重播数据集：收集了专家策略在线训练过程中的全部样本，相当于基于多种策略生成的数据集。

# 算法改进
## 离线学习 vs. 模仿学习
离线学习中通常包含了行为克隆（模仿学习的一种）思想。
模仿学习是针对专家数据集，目标就是尽可能去贴近专家策略，可以认为专家数据集已经是最优，只需要学习出和专家策略相似的策略即可。而离线强化学习则增加了reward，也就是增大能够获得更多奖励的轨迹概率，通常是从次优的数据集中学习策略。

## 通用实现
[CORL: Research-oriented Deep Offline Reinforcement Learning Library](https://github.com/tinkoff-ai/CORL)
八种离线强化学习算法，当下工作集中于已有算法的改进，核心都是解决数据分布偏移导致的Q错误高估计：
- BC、BCQ：行为克隆思想，偏保守，缺乏探索，高度依赖数据集质量。
- TD3+BC：TD3算法基础上引入行为克隆加权和归一化，但依旧依赖数据集质量，更多的是为了加速学习。
- CQL：打压数据集外action的Q-value，激励数据集内action的Q-value。
- IQL：Implicit Q-Learning，引入期望回归避免去估计数据集中不存在的action，和其它算法的思路方向不同。
- AWAC：最小化当前策略和最优策略间的KL散度。
- SAC-N、EDAC：增加必需Q函数数目，通过合理增加Q函数数目达到对Q函数上下限的合理控制。
- Decision Transformer：设定reward目标，相比行为克隆多了有限的泛化能力。

## CQL
[Conservative Q-Learning for Offline Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html)
![image.png](https://s2.loli.net/2023/10/21/UkauHcGXLDZpr7b.png)

## IQL
[Offline Reinforcement Learning with Implicit Q-Learning](http://arxiv.org/abs/2110.06169)
![image.png](https://s2.loli.net/2023/10/21/sUJTMEPSZyBhbCF.png)

## TD3+BC
[A Minimalist Approach to Offline Reinforcement Learning](https://arxiv.org/abs/2106.06860v2)
![image.png](https://s2.loli.net/2023/10/21/vnaE9BpRSACOVY3.png)
