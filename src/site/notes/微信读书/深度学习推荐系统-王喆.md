---
{"dg-publish":true,"permalink":"/微信读书/深度学习推荐系统-王喆/","title":"深度学习推荐系统-王喆","noteIcon":"1","created":"2024-08-11T17:07:11.158+08:00","updated":"2024-09-22T21:42:52.649+08:00"}
---

# 元数据
> [!abstract] 深度学习推荐系统
> - ![ 深度学习推荐系统|200](https://cdn.weread.qq.com/weread/cover/19/cpplatform_szvqqzga1n1nhpndvt4eaq/t7_cpplatform_szvqqzga1n1nhpndvt4eaq1681458384.jpg)
> - 书名： [深度学习推荐系统](https://weread.qq.com/web/reader/b7732f20813ab7c33g015dea)
> - 作者： 王喆
> - 简介： 深度学习在推荐系统领域掀起了一场技术革命，本书从深度学习推荐模型、Embedding技 术、推荐系统工程实现、模型评估体系、业界前沿实践等几个方面介绍了这场技术革命中的主 流技术要点。
> - 出版时间： 2020-03-01
> - ISBN： 9787121384646
> - 分类： #微信读书/科学技术 工业技术
> - 出版社： 电子工业出版社
> - 总字数： 186675
> - 推荐值： 95%
> - 阅读时间： 
> - 开始阅读： 

# 高亮划线

## 1.1 为什么推荐系统是互联网的增长引擎


> [!note]
> 推荐系统的“终极”优化目标应包括两个维度：一个维度是用户体验的优化；另一个维度是满足公司的商业利益。
>> [!quote] 2024-07-29 16:21:56 
## 2.2 协同过滤——经典的推荐算法

 

> [!note]
> 用户的历史数据向量往往非常稀疏，对于只有几次购买或者点击行为的用户来说，找到相似用户的准确度是非常低的，这导致UserCF不适用于那些正反馈获取较困难的应用场景（如酒店预定、大件商品购买等低频应用）。
>> [!quote] 2024-07-29 20:38:56 

> [!note]
> 热门的物品具有很强的头部效应，容易跟大量物品产生相似性；而尾部的物品由于特征向量稀疏，很少与其他物品产生相似性，导致很少被推荐。
>> [!quote] 2024-07-29 20:40:32 
## 2.3 矩阵分解算法——协同过滤的进化


> [!note]
> 矩阵分解在协同过滤算法中“共现矩阵”的基础上，加入了隐向量的概念，加强了模型处理稀疏矩阵的能力，针对性地解决了协同过滤存在的主要问题。
>> [!quote] 2024-07-31 10:30:45 

> [!note]
> 在矩阵分解算法中，由于隐向量的存在，使任意的用户和物品之间都可以得到预测分值。而隐向量的生成过程其实是对共现矩阵进行全局拟合的过程，因此隐向量其实是利用全局信息生成的，有更强的泛化能力；而对协同过滤来说，如果两个用户没有相同的历史行为，两个物品没有相同的人购买，那么这两个用户和两个物品的相似度都将为0（因为协同过滤只能利用用户和物品自己的信息进行相似度计算，这就使协同过滤不具备泛化利用全局信息的能力）。
>> [!quote] 2024-07-31 10:56:41 
## 2.4 逻辑回归——融合多种特征的推荐模型


> [!note]
> 逻辑回归模型将推荐问题转换成了一个点击率(ClickThrough Rate，CTR)预估问题。
>> [!quote] 2024-07-31 11:04:00 

> [!note]
> 除了在形式上适于融合不同特征，形成较“全面”的推荐结果，其流行还有三方面的原因：一是数学含义上的支撑；二是可解释性强；三是工程化的需要。
>> [!quote] 2024-07-31 11:19:36 
## 2.5 从FM到FFM——自动特征交叉的解决方案


> [!note]
> 在对样本集合进行分组研究时，在分组比较中都占优势的一方，在总评中有时反而是失势的一方，这种有悖常理的现象，被称为“辛普森悖论”。
>> [!quote] 2024-07-31 11:20:02 
## 2.6 GBDT+LR——特征工程模型化的开端


> [!note]
> GBDT是由多棵回归树组成的树林，后一棵树以前面树林的结果与真实结果的残差为拟合目标
>> [!quote] 2024-07-31 15:26:19 

> [!note]
> GBDT容易产生过拟合，以及GBDT的特征转换方式实际上丢失了大量特征的数值信息
>> [!quote] 2024-07-31 16:37:43 
## 3.4 NeuralCF模型——CF与深度学习的结合


> [!note]
> NeuralCF 用“多层神经网络+输出层”的结构替代了矩阵分解模型中简单的内积操作
>> [!quote] 2024-08-01 10:38:59 
## 3.5 PNN模型——加强特征交叉能力


> [!note]
> 唯一的区别在于PNN模型用乘积层(Product Layer)代替了Deep Crossing模型中的Stacking层。也就是说，不同特征的Embedding向量不再是简单的拼接，而是用Product操作进行两两交互，更有针对性地获取特征之间的交叉信息。
>> [!quote] 2024-08-01 16:31:13 

> [!note]
> 平均池化的操作经常发生在同类Embedding上，例如，将用户浏览过的多个物品的Embedding进行平均
>> [!quote] 2024-08-01 16:38:31 
## 3.6 Wide＆Deep模型——记忆能力和泛化能力的综合


> [!note]
> Wide部分的主要作用是让模型具有较强的“记忆能力”(memorizat​ion)；Deep部分的主要作用是让模型具有“泛化能力”(general​izat​ion)
>> [!quote] 2024-08-02 09:45:35 

> [!note]
> “记忆能力”可以被理解为模型直接学习并利用历史数据中物品或者特征的“共现频率”的能力
>> [!quote] 2024-08-02 09:47:20 

> [!note]
> “泛化能力”可以被理解为模型传递特征的相关性，以及发掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力。
>> [!quote] 2024-08-02 09:47:36 
## 3.7 FM与深度学习模型的结合


> [!note]
> 针对Embedding层收敛速度的难题，FNN模型的解决思路是用FM模型训练好的各特征隐向量初始化Embedding 层的参数，相当于在初始化神经网络参数时，已经引入了有价值的先验信息
>> [!quote] 2024-08-02 10:08:37 
## 3.8 注意力机制在推荐模型中的应用


> [!note]
> 重，这个权重就代表了“注意力”的强弱
>> [!quote] 2024-08-02 15:50:58 
## 3.9 DIEN——序列模型与推荐系统的结合


> [!note]
> 兴趣抽取层的基本结构是GRU（Gated Recurrent Uni​t，门循环单元）网络。相比传统的序列模型RNN（Recurrent Neural Network，循环神经网络）和LSTM （Long Short-Term Memory，长短期记忆网络）​，GRU解决了RNN的梯度消失问题(Vanishing Gradients Problem)。与LSTM相比，GRU的参数数量更少，训练收敛速度更快，因此成了DIEN序列模型的选择。
>> [!quote] 2024-08-02 16:42:18 
## 4.2 Word2vec——经典的Embedding方法


> [!note]
> 每个词都是由相邻的词决定的（图4-2中CBOW模型的主要原理）​，或者每个词都决定了相邻的词（图4-2中Skip-gram模型的主要原理）​。
>> [!quote] 2024-08-03 15:52:17 
## 5.3 推荐系统的实时性


> [!note]
> 对于物品Embedding的更新，一般需要全局的数据，因此只能在服务器端进行更新；而对用户Embedding来说，则更多依赖用户自身的数据。那么把用户Embedding的更新过程移植到客户端来做，能实时地把用户最近的行为数据反映到用户的Embedding中来，从而可以在客户端通过实时改变用户Embedding的方式完成推荐结果的实时更新。
>> [!quote] 2024-08-05 14:42:25 
## 5.5 推荐系统中比模型结构更重要的是什么


> [!note]
> 对推荐系统效果的改进，最有效的方法不是执着地改进那块已经很长的木板，而是发现那块最短的木板，提高整体的效果。
>> [!quote] 2024-08-05 16:15:09 
## 5.6 冷启动的解决办法


> [!note]
> 通过简单计算可知，当物品的平均回报高时，UCB的得分会高；同时，当物品的曝光次数低时，UCB的得分也会高。也就是说，使用UCB方法进行推荐，推荐系统会倾向于推荐“效果好”或者“冷启动”的物品。那么，随着冷启动的物品有倾向性地被推荐，冷启动物品快速收集反馈数据，使之能够快速通过冷启动阶段。
>> [!quote] 2024-08-05 16:39:33 
## 6.3 推荐模型离线训练之Parameter Server


> [!note]
> Parameter Server实现分布式机器学习模型训练的要点如下：· 用异步非阻断式的分布式梯度下降策略替代同步阻断式的梯度下降策略。· 实现多server节点的架构，避免单master节点带来的带宽瓶颈和内存瓶颈。· 使用一致性哈希、参数范围拉取、参数范围推送等工程手段实现信息的最小传递，避免广播操作带来的全局性网络阻塞和带宽浪费。
>> [!quote] 2024-08-11 09:52:02 
## 6.5 深度学习推荐模型的上线部署


> [!note]
> 复杂网络离线训练、生成Embedding存入内存数据库、线上实现逻辑回归或浅层神经网络等轻量级模型拟合优化目标
>> [!quote] 2024-08-11 10:07:29 
## 7.2 直接评估推荐序列的离线指标


> [!note]
> 其实，还有一种更直观的绘制ROC 曲线的方法。首先，根据样本标签统计出正负样本的数量，假设正样本数量为P，负样本数量为N；接下来，把横轴的刻度间隔设置为1/N，纵轴的刻度间隔设置为1/P；再根据模型输出的预测概率对样本进行排序（从高到低）​；依次遍历样本，同时从零点开始绘制ROC曲线，每遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线，每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线，直到遍历完所有样本，曲线最终停在(1，1)这个点，整个ROC曲线绘制完成。
>> [!quote] 2024-08-11 16:32:40 
## 7.4 A/B测试与线上评估指标


> [!note]
> 可以用两个原则简述A/B测试分层和分流的机制。(1)层与层之间的流量“正交”​。(2)同层之间的流量“互斥”​。
>> [!quote] 2024-08-11 16:39:02 
## 8.1 Facebook的深度学习推荐系统


> [!note]
> 除此之外，少量的点击数据遗漏是不可避免的，这就要求数据平台能够阶段性地对所有数据进行全量重新处理，避免流处理平台产生的误差积累。
>> [!quote] 2024-08-11 16:46:19 
## 8.3 YouTube深度学习视频推荐系统


> [!note]
> 在对训练集的预处理过程中，YouTube没有采用原始的用户日志，而是对每个用户提取等数量的训练样本，这是为什么呢？YouTube这样做的目的是减少高度活跃用户对模型损失的过度影响，使模型过于偏向活跃用户的行为模式，忽略数量更广大的长尾用户的体验。
>> [!quote] 2024-08-11 16:56:39 
# 读书笔记

## 2.2 协同过滤——经典的推荐算法

### 划线评论
- 📌 。相比余弦相似度，皮尔逊相关系数通过使用用户平均分对各独立评分进行修正，减小了用户评分偏置的影响。 
    - 💭 有的用户习惯给高分，有的习惯给低分，消除了这部分的基础影响。
    - ⏱ 2024-07-29 20:36:38
   
## 3.8 注意力机制在推荐模型中的应用

### 划线评论
- 📌 问题的关键在于加和池化(Sum Pool​ing)操作，它相当于“一视同仁”地对待所有交叉特征，不考虑不同特征对结果的影响程度，事实上消解了大量有价值的信息。 
    - 💭 NFM模型需要加入注意力机制的原因。
    - ⏱ 2024-08-02 10:47:05
   
## 3.10 强化学习与推荐系统的结合

### 划线评论
- 📌 竞争梯度下降算法 
    - 💭 增加一个扰动参数ΔW，根据用户反馈确定两个Q网络生成推荐列表的效果，进而确定是否更新W。
    - ⏱ 2024-08-02 17:03:40
   
## 4.4 Graph Embedding——引入更多结构信息的图嵌入技术

### 划线评论
- 📌 同质性相同的物品很可能是同品类、同属性，或者经常被一同购买的商品，而结构性相同的物品则是各品类的爆款、各品类的最佳凑单商品等拥有类似趋势或者结构性属性的商品。 
    - 💭 同质性使用DFS，q越小随机游走到远方节点可能性更大；结构性使用BFS，p越小随机回到上一个节点可能性更大。
    - ⏱ 2024-08-04 09:35:28
   
## 5.4 如何合理设定推荐系统中的优化目标

### 划线评论
- 📌 从模型结构（如图5-11）上看，底层的Embedding层是CVR部分和CTR部分共享的，共享Embedding层的目的主要是解决CVR任务正样本稀疏的问题，利用CTR的数据生成更准确的用户和物品的特征表达。 
    - 💭 转化率和点击率。
    - ⏱ 2024-08-05 16:13:43
   
# 本书评论
